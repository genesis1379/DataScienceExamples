---
title: 'Data Challenge on Synthetic Data Set'
subtitle: 
author  : Giovanni Fossati
job     : null
output  : 
  html_document:
    self_contained: false
---

```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
require(knitr)
options(width = 100, 
        scipen = 1)
opts_chunk$set(message = FALSE, 
               error = FALSE, 
               warning = FALSE, 
               collapse = TRUE, 
               tidy = FALSE,
               cache = FALSE, 
               cache.path = '.cache/', 
               comment = '#',
               fig.align = 'center', 
               dpi = 100, 
               fig.path = 'figures/')
library("dplyr")
library("tidyr")
library("magrittr")
library("ggplot2")
library("gridExtra")

library("MASS")
library("caret")
library("earth")
```

```{r load_packages, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
source("./scripts/my_functions_miscellaneous.R")
source("./scripts/my_functions_plot.R")
source("./scripts/my_functions_modeling.R")
```


# SUMMARY

This is the analysis of a _synthetic dataset_ provided for a _data challenge_.

Before focusing on model building, we had to tackle the issue of missing data, which,
while not being in aggregate a large fraction of the data set, are distributed in such
a way that ignoring incomplete observations was not an option. 

Based on a quick review of the data, we made the assumption that the data were missing
completely at random, and so we deemed it acceptable to adopt a simple approach for imputing them,
mostly based on random sampling from the existing data for each variable. 

The model we adopted in the end is _Multivariate Adaptive Regression Splines_ (MARS). 
It offers an attractive combination of light computational "cost", flexibility, built-in variable
selection, interpretatibility, and on the _Training_ data set it gave good results (fitting
included _cross validation_).
On the _Training_ data sets these models yield $MSE \simeq 2.4$ and $R^2 \simeq 0.91$.

We computed predictions of best-fit models on four different imputed _Training_ data, on each of
four different imputed _Testing_ data sets, and then averaged their predictions to obtain our
prediction for submission.

One reason of concern is that the statistical properties of the data seem to slightly differ
between the _Training_ and the _Testing_ data sets.
On the other hand, considering that the model is using only about a dozen variables, it is possible
that its performance may not be so seriously affected by this apparent difference.

Read also [these additional thoughts](#FINAL_THOUGHTS).


### Note on the Report _Reproducible_ Format

This report is generated from a _Rmarkdown_ document which includes all code to perform
data processing, modeling, plotting, etc. (except for functions defined in external scripts, 
included in the archive).
However, for readability only a few bits of code are _echoed_ explicitly in the compiled document.
The full straight reproducibility is only limited by the fact that in the interest of simplicity
and for computational convenience some parts of the processing have been flagged as _inactive_ 
(`eval = FALSE`), and in its current form some data are instead loaded from previously saved work,
namely the model fitting. 
That said, the document includes the code to perform the entire analysis, and a few changes would
allow to do so by compiling it.

The source files are posted on [GitHub](https://github.com/pedrosan/DataScienceExamples/tree/master/Synthetic_Data/)

### Outline

* [DATA LOADING AND SET UP](#data_loading)
* [EXPLORATORY ANALYSIS](#EDA)
    * [Missing Data](#missing_data)
    * [Summary of Numeric Variables](#summary_numeric)
    * [Summary of Non-Numeric Variables](#summary_non_numeric)
    * [The _target_ Variable](#target_variable)
* [DATA IMPUTATION](#IMPUTATION)
* [MODELING](#MODELING)
    * [Linear Regression](#linear_regression)
    * [_Multivariate Adaptive Regression Splines_ (MARS)](#MARS)
    * [Predicting the _Testing_ Data Set](#Predictions)
* [FINAL THOUGHTS](#FINAL_THOUGHTS)

* [APPENDIX](#APPENDIX)
    * [Statistics of `NA`](#Appendix_NA)
    * [Linear Regression Model Summary (Fit #4)](#Appendix_LM)
    * [MARS Models Summary](#Appendix_MARS)
    * [User defined functions](#APPENDIX_user_functions)
    * [R Session Info](#SessionInfo)



<hr class="thin_separator">
<a name="data_loading"></a>

# DATA LOADING AND SET UP

```{r loading_data, cache = TRUE}
train <- read.delim(file = "data/training.txt.gz", stringsAsFactors = FALSE)
test  <- read.delim(file = "data/testing.txt.gz", stringsAsFactors = FALSE)
```

Beside the `target` variable that is _`r class(train[, 1])`_, 
the data sets comprise __`r ncol(test)`__ variables, with the following breakdown by type:

```{r check_variable_types}
table(sapply(train[, -1], class))
```

The non-numeric variables are: __` `r colnames(train)[!sapply(train, is.numeric)]` `__.
They are all of  of _character_ type and look like _bona fide_ factor variables, non-ordered.   
I will refer to them as _factors_ or _character variables_ interchangeably.

For convenience of processing I defined flags for character/non-character variables and vectors 
of indices to be able to directly select them when necessary to handle numeric and character
variables separately.

```{r define_flags_for_non_numeric_variables}
flag_factor_cols <- sapply(train[, -1], is.character)
idx_factors <- (2:ncol(train))[flag_factor_cols]
idx_numeric <- (2:ncol(train))[!flag_factor_cols]
factor_cols_names <- colnames(train)[idx_factors]
```


#### Fixing Empty Strings as `NA` in the Character Variables

Missing data in character variables are not directly translated into `NA` but treated simply as empty strings.   
Assuming that they are in fact missing data, we need to change them to be proper NAs.

```{r fix_empty_strings}
# Training set
for( s in factor_cols_names) { train[train[, s] == "", s] <- NA }

# Testing set
for( s in factor_cols_names) { test[test[, s] == "", s] <- NA }
```

```{r check_NA-train, eval = FALSE, echo = FALSE}
# Check/count NAs in factor variables in the _Training_ set:
sapply(train[, factor_cols_names], function(x) table(x, useNA = "always"))
```

```{r check_NA-test, eval = FALSE, echo = FALSE}
# and in the _Testing_ set:
sapply(test[, factor_cols_names], function(x) table(x, useNA = "always"))
```


<hr class="thin_separator">
<a name="EDA"></a>

# EXPLORATORY ANALYSIS

<a name="missing_data"></a>

## Missing Data

First step is to review how missing data are distributed in the data set, 
starting with how many they are.   
__NOTE__: tables showing these statistics in more detail are included in the [APPENDIX](#APPENDIX).


### _Training_ Data Set

<button class="toggle_code">show code</button>
```{r stats_NA_in_train-compute, echo = TRUE, cache = TRUE}
train.NA_by_column <- sapply(train[, -1], function(x) {sum(is.na(x))})
train.NA_by_row <- apply(train[, -1], 1, function(x) sum(is.na(x)))
train.sum_NA <- sum(is.na(train[, -1]))
train.avrg_NA_by_column <- train.sum_NA/ncol(train[, -1])
train.pct_NA_by_column <- 100*train.avrg_NA_by_column/nrow(train[, -1])
train.avrg_NA_by_row <- train.sum_NA/nrow(train[, -1])
train.pct_NA_by_row <- 100*train.avrg_NA_by_row/ncol(train[, -1])
train.clean_columns <- sum(train.NA_by_column == 0)
train.clean_rows <- sum(train.NA_by_row == 0)
```

* In the _Training_ set there are __overall__ __`r train.sum_NA`__ `NA`,
out of __$`r nrow(train)*(ncol(train)-1)`$__ data points, 
_i.e._ about __`r round(100*train.sum_NA/(nrow(train)*(ncol(train)-1)), 2)`%__ of the total data.

* __Summary by Columns (i.e. Variables)__:
    * There are __`r sum(train.NA_by_column == 0)`__ variables without `NA`.
    * On average variables have around __`r round(train.avrg_NA_by_column, 1)`__ 
      (_i.e._ __`r round(train.pct_NA_by_column, 2)`%__ of `r nrow(train)` observations).


* __Summary by Rows (i.e. Observations)__:

    * There are __`r sum(train.NA_by_row == 0)`__ observations exempt from `NA`, 
      just about __`r round(100*sum(train.NA_by_row == 0)/nrow(train), 2)`%__.
    * On average observations have around __`r round(train.avrg_NA_by_row, 1)`__ 
      (_i.e._ __`r round(train.pct_NA_by_row, 2)`%__ of `r ncol(train[, -1])` variables).


### _Testing_ Data Set

<button class="toggle_code">show code</button>
```{r stats_NA_in_test-compute, echo = TRUE, cache = TRUE}
test.NA_by_column <- sapply(test, function(x) {sum(is.na(x))})
test.NA_by_row <- apply(test, 1, function(x) sum(is.na(x)))
test.sum_NA <- sum(is.na(test))
test.avrg_NA_by_column <- test.sum_NA/ncol(test)
test.pct_NA_by_column <- 100*test.avrg_NA_by_column/nrow(test)
test.avrg_NA_by_row <- test.sum_NA/nrow(test)
test.pct_NA_by_row <- 100*test.avrg_NA_by_row/ncol(test)
test.clean_columns <- sum(test.NA_by_column == 0)
test.clean_rows <- sum(test.NA_by_row == 0)
```

* The _Testing_ data set that contains __`r test.sum_NA`__ `NA`, out of __$`r nrow(test)*(ncol(test)-1)`$__ 
total data points, _i.e._ about __`r round(100*test.sum_NA/(nrow(test)*(ncol(test)-1)), 2)`%__.

* __Summary by Columns (i.e. Variables)__:
    * There are __`r sum(test.NA_by_column == 0)`__ variables without NA.
    * On average variables have around __`r round(test.avrg_NA_by_column, 1)`__ 
      (_i.e._ __`r round(100*test.avrg_NA_by_column/nrow(test), 2)`%__ of `r nrow(test)` observations).


* __Summary by Rows (i.e. Observations)__:
    * There are __`r sum(test.NA_by_row == 0)`__ observations exempt from `NA`, 
      just about __`r round( 100*sum(test.NA_by_row == 0)/nrow(test), 2)`%__.
    * On average observations have around __`r round(test.avrg_NA_by_row, 1)`__ 
      (_i.e._ __`r round(100*test.avrg_NA_by_row/ncol(test), 2)`%__ of `r ncol(test)` variables).


<button class="toggle_code">show code</button>
```{r NA_stats_table, echo = TRUE}
train.NA_stats <- c(train.sum_NA, 
                    nrow(train), 
                    train.clean_columns, 
                    train.avrg_NA_by_column,
                    train.pct_NA_by_column,
                    train.clean_rows,
                    train.avrg_NA_by_row,
                    train.pct_NA_by_row)

test.NA_stats <- c(test.sum_NA, 
                   nrow(test), 
                   test.clean_columns, 
                   test.avrg_NA_by_column,
                   test.pct_NA_by_column,
                   test.clean_rows,
                   test.avrg_NA_by_row,
                   test.pct_NA_by_row)

df.NA_stats <- as.data.frame( matrix(c(train.NA_stats, test.NA_stats), nrow = 2, byrow = TRUE), 
                              row.names = c("Training", "Testing") )

colnames(df.NA_stats) <- c("n_NA", "n_Obs", 
                           "compl_col", "avrg_NA_by_col", "pct_NA_by_col", 
                           "compl_row", "avrg_NA_by_row", "pct_NA_by_row")
```

```{r NA_stats_table-print, echo = FALSE}
kable(df.NA_stats, digits = 2, align = "c", 
      caption = "Summary of NA Statistics by Data Set, Variables, Observations", 
      col.names = c("n_NA", "_Obs", 
                    "compl cols", "NA/col (N)", "NA/col (%)", 
                    "compl rows", "NA/row (N)", "NA/row (%)"))
```


### Visual comparison of `NA` statistics

These plots show that on aggregate the _Training_ and _Testing_ sets are affected by `NA` in similar ways.   
This does not imply that the detailed distribution of `NA` in rows and columns may not be biased
by some trend that would not be revealed by this plots.    

<button class="toggle_plot_code">show plot code</button>
```{r plot_NA_stats-1, echo = TRUE, fig.width = 6, fig.height = 6}
hstep <- 0.0025
# par(mfrow = c(2, 2))
# par(oma = c(0 ,0, 1, 0))
single_panel_mar <- c(2, 2, 2, 1)
single_panel_oma <- c(0 ,0, 2, 0)
gr_par <- list( mar = single_panel_mar, oma = single_panel_oma, 
                cex = 1.0, cex.axis = 0.8, cex.lab = 1.0, 
                las = 0, mgp = c(1.0, 0.0, 0),
                tcl = 0.3)

par(gr_par)

mat.layout <- matrix(1:4, nrow = 2, byrow = TRUE)
layout(mat.layout)

# n_NA/n_TOT , more comparable with testing set
hist(train.NA_by_column/nrow(train), breaks = seq(0.0, 0.1, by = hstep), 
     xlim = c(0.0, 0.05), 
     col = "orange", 
     xlab = "Fraction of missing data (NA)", 
     main = "Training : by Predictor")

hist(train.NA_by_row, freq = FALSE, breaks = seq(0, 20, by = 1), 
     xlim = c(0, 20), 
     col = "orange",
     xlab = "Number of NA in rows", 
     main = "Training : by Observation")

# n_NA/n_TOT , more comparable with train set
hist(test.NA_by_column/nrow(test), breaks = seq(0.0, 0.1, by = hstep), 
     xlim = c(0.0, 0.05), 
     col = "cadetblue",
     xlab = "Fraction of missing data (NA)", 
     main = "Testing : by Predictor")

hist(test.NA_by_row, freq = FALSE, breaks = seq(0, 20, by = 1), 
     xlim = c(0, 20), 
     col = "cadetblue", 
     xlab = "Number of NA in rows", 
     main = "Testing : by Observation")

par(fig = c(0, 1, 0, 1), mar = c(5, 4, 4, 1)+0.1, cex.axis = 1.0)
title("Distributions of the Quantity/Fraction of Missing Data", cex.main = 1.25, outer = TRUE)
```


<hr class="thin_separator">
<a name="summary_numeric"></a>

## Summary of Numeric Variables

A quick visual review of the distribution of the values of each variable shows that 
they are all approximately similar, Gaussian-looking (roughly speaking).    
Because of this, their distributions can be characterized reasonably well by standard
simple summary statistics for each variable.    
We prepare a data frame with the basic summary statistics for each numeric variable (from
`summary()`), adding _interquartile range_ and _standard deviation_.

<button class="toggle_code">show code</button>
```{r summary_stats-train-prepare}
train.stats_by_column <- as.data.frame(t(sapply(train[, idx_numeric], summary)))
colnames(train.stats_by_column) <- c("min", "q025", "median", "mean", "q075", "max", "nNA")

# add IQR
train.stats_by_column$IQR <- (train.stats_by_column$q075 - train.stats_by_column$q025)

# add std.dev.
train.stats_by_column$sd <- sapply(train[, idx_numeric], function(x) sd(x, na.rm = TRUE))
```

For illustration this is a random sample of rows from the resulting data frame:
```{r summary_stats-head, echo = FALSE}
kable( train.stats_by_column[sort(sample(1:nrow(train.stats_by_column), 6)),], digits = 3,
       caption = "Example of Summary Statistics")
```

Same preparation for the _Testing_ set:

<button class="toggle_code">show code</button>
```{r summary_stats-test-prepare}
test.stats_by_column <- as.data.frame(t(sapply(test[, idx_numeric-1], summary)))
colnames(test.stats_by_column) <- c("min", "q025", "median", "mean", "q075", "max", "nNA")

test.stats_by_column$IQR <- (test.stats_by_column$q075 - test.stats_by_column$q025)
test.stats_by_column$sd <- sapply(test[, idx_numeric-1], function(x) sd(x, na.rm = TRUE))
```

<button class="toggle_code">show code</button>
```{r summary_stats-combine_train_and_test}
all.stats_by_column <- bind_rows(train.stats_by_column, test.stats_by_column)
all.stats_by_column$set <- c(rep("train", 250), rep("test", 250))
```

### Distributions of Summary Statistics of Predictors

First we inspect visually the distribution of the summary statistics for the 
`r length(idx_numeric)` numeric variables.   
The following figures show the density curves for median, 0.25 quantile, 0.75
quantile and interquartile range, overlaying the curves for the _Training_ (orange/yellow) and the
_Testing_ (blue) data sets.

<button class="toggle_plot_code">show plot code</button>
```{r plot_predictors_distributions_3x2-train_and_test, fig.width = 9, fig.height = 6, echo = TRUE}
p1 <- ggcompare_stats_distributions(data = all.stats_by_column, var = "median", xlim = c(-0.25, 0.25))
p2 <- ggcompare_stats_distributions(data = all.stats_by_column, var = "q025", xlim = c(-0.85, -0.35))
p3 <- ggcompare_stats_distributions(data = all.stats_by_column, var = "q075", xlim = c(0.35, 0.85))
p4 <- ggcompare_stats_distributions(data = all.stats_by_column, var = "IQR", xlim = c(0.85, 1.55))

grid.arrange(p1, p2, p4, p3, nrow = 2, main = "Comparison of Summary Statistics of Numeric Variables")
```

The two sets of plots reveal an issue that has the potential of affecting negatively the
predictive power of a model built (trained) on the _Training_ set when applied to this _Testing_ set:
__the distribution of some of these summary statistics differ between _Training_ and _Testing_ data sets__, 
namely they are systematically broader in the _Testing_ set than they are in the _Training_ set.

The fact that the distributions of _medians_ and _quantiles_ are different raises some concern for the modeling
To review this matter further we can look at scatter plots of the summary statistics, shown below.

<button class="toggle_plot_code">show plot code</button>
```{r plot_predictors_summary_stats_train_vs_test_v2, fig.width = 7, fig.height = 6, echo = TRUE}
par(fig = c(0, 1, 0, 1))
single_panel_mar <- c(2, 2, 2, 1)
single_panel_oma <- c(0 ,0, 2, 0)
gr_par <- list( mar = single_panel_mar, oma = single_panel_oma, 
                cex = 1.0, cex.axis = 0.8, cex.lab = 1.0, 
                las = 0, mgp = c(1.0, 0.0, 0),
                tcl = 0.3)

mat.layout <- matrix(1:4, nrow = 2, byrow = FALSE)
layout(mat.layout)

test_flag <- rep(FALSE, length(idx_numeric))
par(gr_par)
plot_stats_vs_stats(data1 = train.stats_by_column, data2 = test.stats_by_column, var = "median", flag = test_flag)
plot_stats_vs_stats(data1 = train.stats_by_column, data2 = test.stats_by_column, var = "IQR", flag = test_flag)
plot_stats_vs_stats(data1 = train.stats_by_column, data2 = test.stats_by_column, var = "q025", flag = test_flag)
plot_stats_vs_stats(data1 = train.stats_by_column, data2 = test.stats_by_column, var = "q075", flag = test_flag)
    
par(fig = c(0, 1, 0, 1), mar = c(5, 4, 4, 1)+0.1, cex.axis = 1.0)
title("Comparison of Summary Statistics of Numeric Variables", cex.main = 1.2, outer = TRUE, line = 0)
```

The scatter plots illustrate clearly the different extension of the scatter in the _Training_ and
_Testing_ sets.    

As a side note, there are a handful of outliers, particularly noticeable in quantile plots.  
Before modeling it is hard to tell if these will turn out to be important variables, but they
certainly stand out in these plots.
We can pick them out by filter the data frame on the `q075` value:

```{r outlier, echo = FALSE}
kable( subset(train.stats_by_column, q075 < 0.55), digits = 3, caption = "Outlying Variables")
```

---

### A Thought About Modeling

Thinking about modeling, the best we can wish for is that the most important predictors will
be "well behaved", in the sense that they will be among the variables whose distributions 
are consistent between _Training_ and _Testing_ sets.


<hr class="thin_separator">
<a name="summary_non_numeric"></a>

## Summary of Non-Numeric Variables

Four of the predictors are factors, and we can compare them between _Training_ and _Testing_ set 
by looking at the distribution of the data over their levels.
We first computed the fraction of data in each level of a factor, in the two sets separately,
and then took the ratio (_Training_/_Testing_).
The following barplots show these ratios for the four factor variables.
Horizontal lines mark the 0.9, 1.0, and 1.1 points.

In all variables there are variations up to 10% (larger for `f_121`), which raise another
red flag for modeling.
For instance, if a model considered important levels _A_ and _C_ of `f_121`, the fact that
_A_ is 10% less populated in the _Training_ set, and _C_ 15% more, will affect negatively the
quality of the predictions on the _Testing_ set.

<button class="toggle_plot_code">show plot code</button>
```{r plot_distributions_of_factors_train_vs_test, fig.width = 6, fig.height = 6, echo = TRUE}
factors_table_train <- sapply(train[, factor_cols_names], function(x) table(x, useNA = "always")/length(x))
factors_table_test  <- sapply(test[, factor_cols_names], function(x) table(x, useNA = "always")/length(x))

par(fig = c(0, 1, 0, 1))
single_panel_mar <- c(3, 3, 3, 1)
single_panel_oma <- c(0 ,0, 1, 0)

mat.layout1 <- matrix(1:4, nrow = 2, byrow = TRUE)
layout(mat.layout1)

par(mar = single_panel_mar, oma = single_panel_oma, cex.axis = 1.0, cex.main = 1.0)
barplot((factors_table_train$f_61/factors_table_test$f_61)[1:5], col = "palegoldenrod", ylim = c(0, 1.1), main = "f_61")
abline(h=1.0, col = "red2", lty = 2, lwd = 3)
abline(h=1.1, col = "red2", lty = 3, lwd = 2)
abline(h=0.9, col = "red2", lty = 3, lwd = 2)

barplot((factors_table_train$f_121/factors_table_test$f_121)[1:6], col = "palegoldenrod", ylim = c(0, 1.1), main = "f_121")
abline(h=1.0, col = "red2", lty = 2, lwd = 3)
abline(h=1.1, col = "red2", lty = 3, lwd = 2)
abline(h=0.9, col = "red2", lty = 3, lwd = 2)

barplot((factors_table_train$f_215/factors_table_test$f_215)[1:4], col = "palegoldenrod", ylim = c(0, 1.1), main = "f_215")
abline(h=1.0, col = "red2", lty = 2, lwd = 3)
abline(h=1.1, col = "red2", lty = 3, lwd = 2)
abline(h=0.9, col = "red2", lty = 3, lwd = 2)

barplot((factors_table_train$f_237/factors_table_test$f_237)[1:3], col = "palegoldenrod", ylim = c(0, 1.1), main = "f_237")
abline(h=1.0, col = "red2", lty = 2, lwd = 3)
abline(h=1.1, col = "red2", lty = 3, lwd = 2)
abline(h=0.9, col = "red2", lty = 3, lwd = 2)

par(fig = c(0, 1, 0, 1), mar = c(5, 4, 4, 1) + 0.1, cex.axis = 1.0)
title("Ratio of Level Distributions of Training and Testing Sets", cex.main = 1.2, outer = TRUE, line = 0)
```

<hr class="thin_separator">
<a name="target_variable"></a>

## The `target` Variable 

### Distribution of the Values of `target`.   

(Please note that in order to increase the visibility of the low-count parts the
vertical scale is logarithmic ($log10()$).)

<button class="toggle_plot_code">show plot code</button>
```{r plot_target_distribution_v2, fig.width = 5, fig.height = 3.5, echo = TRUE}
gr_par <- list( mar = single_panel_mar, oma = single_panel_oma, 
                cex = 1.0, cex.axis = 0.8, cex.lab = 1.0, 
                las = 0, mgp = c(1.0, 0.0, 0),
                tcl = 0.3)
par(gr_par)

col_hist <- "salmon"
hh <- hist(train$target, breaks = seq(-40, 40, by = 1.0), plot = FALSE) 
flag_pos <- hh$counts > 0
hhy <- ifelse(hh$counts >0, log10(hh$counts), 0)

plot_filled_histogram(x = hh$breaks, y = c(hhy, 0), 
                      xlim = c(-28, 28), 
                      colors = c("black", "salmon"), 
                      xlab = "target", 
                      ylab = "log10(counts)", 
                      main = "Distribution of Values of 'target' ")
grid()
```


### Correlations b/w `target` and Predictors (and between Predictors)

Once again we treat factor variables separately.   
We compute the correlation matrix with the option dropping `NA` only on a _pair wise_ basis
to preserve as much as possible the integrity of the result, as in this case we want to
know the strength of the correlation between each two variables independently on the others.

<button class="toggle_code">show code</button>
```{r corr_matrix, cache = TRUE, echo = TRUE}
corr1_pcomp <- cor(train[, idx_numeric], use = "pairwise.complete.obs")
corr2_pcomp <- cor(train[, c(1, idx_numeric)], use = "pairwise.complete.obs")
corr1_pcomp_test <- cor(test[, idx_numeric-1], use = "pairwise.complete.obs")

# deduplicated_corr_matrix_as_vector
corr1_pcomp_dedup <- data.frame(corr = (corr1_pcomp[upper.tri(corr1_pcomp, diag = FALSE)]))
corr1_pcomp_test_dedup <- data.frame(corr = (corr1_pcomp_test[upper.tri(corr1_pcomp_test, diag = FALSE)]))
```

We summarize the unwiedly correlation matrices in following histograms plots: left to right:

* predictor-`target` correlation in the _Training_ set, 
* predictor-predictor correlation in the _Training_ set,
* predictor-predictor correlation in the _Testing_ set.

To highlight the low-frequency cases, the vertical scale is truncated.

<button class="toggle_plot_code">show plot code</button>
```{r plot_correlation_stats, fig.width = 9, fig.height = 4, echo = TRUE}
par(gr_par)

mat.layout <- matrix(1:3, nrow = 1, byrow = TRUE)
layout(mat.layout)

col_hist <- "salmon"
hist(corr2_pcomp[1, -1], freq = TRUE, breaks = seq(-1.0, 1.0, by = 0.02), 
     xlim = c(-0.2, 1.0), 
     ylim = c(0, 10),
     col = col_hist, 
     xlab = "Correlation b/w 'target' and 'predictors'",
     main = "Training - 'Zoomed' Distribution of\nCor(target, predictor)", cex.main = 1.2)

col_hist <- "thistle3"
hist(corr1_pcomp_dedup$corr, freq = TRUE, breaks = seq(-1.0, 1.0, by = 0.02), 
     xlim = c(-0.2, 1.0), 
     ylim = c(0, 10),
     col = col_hist, 
     xlab = "Correlation b/w 'predictors'",
     main = "Training - 'Zoomed' Distribution of\nCor(predictor, predictor)", cex.main = 1.2)

col_hist <- "cadetblue"
hist(corr1_pcomp_test_dedup$corr, freq = TRUE, breaks = seq(-1.0, 1.0, by = 0.02), 
     xlim = c(-0.2, 1.0), 
     ylim = c(0, 10),
     col = col_hist, 
     xlab = "Correlation b/w 'predictors'",
     main = "Testing - 'Zoomed' Distribution of\nCor(target, predictor)", cex.main = 1.2)

par(fig = c(0, 1, 0, 1), mar = c(5, 4, 4, 1)+0.1, cex.axis = 1.0)
title("Distributions of Correlation Strengths b/w Variables", cex.main = 1.2, outer = TRUE, line = 0)
```

<button class="toggle_code">show code</button>
```{r correlated_variables, echo = TRUE}
test_corr1_pcomp <- corr1_pcomp
diag(test_corr1_pcomp) <- 0.0
train.correlated_variables <- row.names(which(abs(test_corr1_pcomp) > 0.6, arr.ind = TRUE)) %>% unique(.)

test_corr1_pcomp_test <- corr1_pcomp_test
diag(test_corr1_pcomp_test) <- 0.0
test.correlated_variables <- row.names(which(abs(test_corr1_pcomp_test) > 0.6, arr.ind = TRUE)) %>% unique(.)

train.correlated_with_target <- names(which(abs(corr2_pcomp[1, ]) > 0.1))[-1]
```

There is very little correlation between variables in the _Training_ data set, with a pile up in a
narrow peak around 0 and only a handful of variables with stronger correlation, $\sim 0.7$.
In this case the _Testing_ set exhibits very similar properties.

* Five pairs of predictors stand out at relatively high correlation strength, $\sim 0.7$, 
  involving the same 10 variables in the _Training_ and _Testing_ data sets:
    * ` `r train.correlated_variables` `
    * ` `r test.correlated_variables` `

* About half a dozen predictors are weakly/mildly correlated with `target`:
    * ` `r train.correlated_with_target` `

In view of model building this findings would seem to indicate that we can expect 
that it will possible to model the outcome with a fairly limited subset of the predictors.
We must keep in mind that the Pearson method is poorly suited for assessing relationship between
variables for a wide range of scenario, but after reviewing scatterplots of the variables 
we think that in this case it may provide a reliable measure of correlation.


### Scatterplots of `target` vs. Predictors

We visually inspected the scatterplots between `target` and all variables and they all look like
smooth clouds with a central density peak, with in a few cases some elongation or substructures
(_e.g._ the top right panel below).
For illustrative purposes we would like to show a few examples of scatterplots for the `target`
variable and predictors, for four different levels of correlation (see plot titles).    

<button class="toggle_plot_code">show plot code</button>
```{r scatterplot_target_vs_predictors_v2, fig.width = 8, fig.height = 6, echo = TRUE}
par(fig = c(0, 1, 0, 1))
single_panel_mar <- c(2, 2, 2, 1)
single_panel_oma <- c(0 ,0, 2, 0)
gr_par <- list( mar = single_panel_mar, oma = single_panel_oma, 
                cex = 1.0, cex.axis = 0.8, cex.lab = 1.0, 
                las = 0, mgp = c(1.0, 0.0, 0),
                tcl = 0.3)

mat.layout <- matrix(1:6, nrow = 2, byrow = TRUE)
layout(mat.layout)

test_flag <- rep(FALSE, length(idx_numeric))
par(gr_par)
plot_target_vs_variable(data = train, var = "f_143")
plot_target_vs_variable(data = train, var = "f_13")
plot_target_vs_variable(data = train, var = "f_94")
plot_target_vs_variable(data = train, var = "f_35")
plot_target_vs_variable(data = train, var = "f_161")
plot_target_vs_variable(data = train, var = "f_175")

par(fig = c(0, 1, 0, 1), mar = c(5, 4, 4, 1)+0.1, cex.axis = 1.0)
title("Examples of Scatterplots of 'target' vs. 'variable'", cex.main = 1.3, outer = TRUE, line = 1)
```

### Distribution of `target` Values by Factor Levels

The last check concerns possible relationships between the `target` variable 
and levels of the four factor variables.
Boxplots are well suited for this purpose and we show the four of them below.

<button class="toggle_code">show code</button>
```{r outcome_vs_factors, echo = TRUE}
df_outcome_and_factors <- train[, c(1, idx_factors)]
for( s in factor_cols_names ) {
    tmp_names <- names(table(df_outcome_and_factors[, s]))
    df_outcome_and_factors[is.na(df_outcome_and_factors[, s]), s] <- "NA"
    df_outcome_and_factors[, s] <- factor(df_outcome_and_factors[, s], levels = c("NA", sort(tmp_names, decreasing = TRUE)), ordered = TRUE)
}
```

<button class="toggle_plot_code">show plot code</button>
```{r figure-boxplot_outcome_vs_factors, fig.width = 8, fig.height = 6, echo = TRUE}
par(fig = c(0, 1, 0, 1))
single_panel_mar <- c(3, 4, 2, 1)
single_panel_oma <- c(0 ,0, 2, 0)
gr_par <- list(mar = single_panel_mar, oma = single_panel_oma, 
               cex = 1.0, cex.axis = 1.0, cex.lab = 1.2, 
               las = 0, mgp = c(1.3, 0.5, 0),
               tcl = 0.3)

mat.layout <- matrix(1:4, nrow = 2, byrow = TRUE)
layout(mat.layout)

par(gr_par)
boxplot_target_for_factor(data = df_outcome_and_factors, factor_name = "f_61")
boxplot_target_for_factor(data = df_outcome_and_factors, factor_name = "f_121")
boxplot_target_for_factor(data = df_outcome_and_factors, factor_name = "f_215")
boxplot_target_for_factor(data = df_outcome_and_factors, factor_name = "f_237")

par(fig = c(0, 1, 0, 1), mar = c(5, 4, 4, 1)+0.1, cex.axis = 1.0)
title("Boxplots of 'target' vs factor levels", cex.main = 1.3, outer = TRUE, line = 0)
```

A quick look at the boxplots suggest that: 

* `f_121` and `f_215` may not be important variables _per se_ (but they could still have an effect
  through interaction with other variables).
* `f_61` and `f_237` may instead be picked up in a model because there are shifts of `target`
  values from level to level, albeit not large.


<hr class="thin_separator">
<a name="IMPUTATION"></a>

# DATA IMPUTATION 

Given the characteristics of the missing data it is not possible to adopt the simplest
approach, that is to retain only complete observations.
Doing so would result in working with only less than 1% of the data.
It is therefore necessary to _impute_ the missing data.

### General Considerations

The first question would concern the cause of this _missingness_, and unfortunately for this data
set we have no information about the origin of the data and their meaning.
Usually this latter would be "available" but in this case the data are simulated, hence it is
unlikely that they had any particular _interpretable_ meaning.

Broadly speaking there are a few missingness mechanisms.  

* __Missing completely at random__: the probability of missingness is the same for every data point.
* __Missing at random__: the probability of a missing a data point depends only on available information, 
   _i.e._ observed variables.
* __Missingness depends on unobserved variables__: this is the case when the probability dependes on information 
   that is not available.   Data are not missing at random.  
* __Missingness depends on the missing value itself__, commonly called _censoring_:

If missingness is not at random it should be modeled, and the difficulty of this task depends
on the mechanism behind it.


### Imputation Strategy

On the basis of our quick look at the data it would seem reasonable to __hypothesize that 
data are missing completely at random__.  
There does not seem to be a pattern in the missing data, and the fact that there is very little
correlation between predictors suggests that it is unlikely that the data missing in a variable
depend on values of other variables.   
In principle, if this was indeed the case, excluding incomplete cases would not bias the data set and in
turn our modeling.
Unfortunately, as noted above, we can not take this approach and we have to impute the missing data.

For _numeric variables_ we proceeded in the following way:

* Variables are imputed one at a time.
* For each variable $x_i$ we first look for variables with correlation with it greater than a threshold value (generously low).
* If there are any correlated variables, $\{x_j\}$, we check if in this reduced data set $x_i + \{x_j\}$ there are observations comprising just `NA`.
    * If there are, we drop from $\{x_j\}$ the least correlated variable and 
    * check again, until we are left with just $x_i$ and one of its correlated variables.
* If after this vetting, at least one $\{x_j\}$ is left, we impute the missing values of $x_i$ by means of
  a linear regression model with the $\{x_j\}$ as predictors.
    * Since among a handful of variables the worst case scenario for the distribution of `NA`s would cause 
      the loss of only a few percent of the observations, the fact that `lm()` only keeps complete cases is 
      not likely to have a significant impact.
* If not enough data are left, we impute the missing data of $x_i$ by random sampling the observed ones
  (operation performed by the function `impute_with_random_sampling()`.

_Non-numeric variables_ were imputed simply by random sampling of the observed values.


### Doing the Imputation

Data are imputed one variable at a time, starting from the _numeric variables_.
We know from the previous analysis that variables are really weakly correlated at best, and so we
set a pretty low value to the threshold on correlation strength for considering variable for
imputation by the linear regression. 

Given that the imputation is mostly done by random re-sampling, each time we impute the missing data
we get a data set different from another one on $~2$% of values. 

To improve the robustness of the model building and testing tasks, we produced several different
imputed data sets for both _Training_ and _Testing_ set 
(for reproducibility we then set the seed before imputation.)

```{r impute_train_data_set, cache = TRUE}
# First create correlation matrix and set its diagonal elements to 0.0 to avoid picking them up
# when filter for pairs with high correlation strength.
corr_pcomp <- cor(train[, idx_numeric], use = "pairwise.complete.obs")
diag(corr_pcomp) <- 0.0

threshold <- 0.05
seeds.train_impute <- c(2931, 7777, 4853, 6464)

for(s in seeds.train_impute) {
    train_imputed <- train
    set.seed(s)

    # Imputation of _numeric_ variables:
    for(i in idx_numeric) {
        yname <- colnames(train)[i]
        check <- sum(abs(corr_pcomp[yname, ]) > threshold)
    
        if(check == 0) {
            train_imputed[, i] <- impute_with_random_sampling(data = train[, i])
        } else {
            train_imputed[, i] <- impute_with_lm(data = train, 
                                                corr_matrix = corr_pcomp, 
                                                i = i, 
                                                threshold = threshold, 
                                                verbose = FALSE)
        }
    }
    # Imputation of _non-numeric_ (i.e. factor) variables:
    for(i in idx_factors) {
        train_imputed[, i] <- impute_with_random_sampling(data = train[, i])
    }

    assign(paste0("train_imputed_", s), train_imputed)
}
```

__NOTE:__  _for convenience instead of performing the imputation in the script directly, although it 
is coded for it, we saved the imputed data sets and load them invisibly to be able to use
them in the following sections.  The same applies to the model fits._

```{r impute_train-check_imputation, echo = FALSE, eval = FALSE}
# We can verify if there are any variables still missing some data points:
table(sapply(train_imputed, function(x) sum(is.na(x))), useNA = "always")
```

### Imputation of the _Testing_ Data Set

The same procedure and functions were applied to the _Testing_ data set.

<button class="toggle_code">show code</button>
```{r impute_test_data_set, cache = TRUE}
corr_pcomp_test <- cor(test[, (idx_numeric-1)], use = "pairwise.complete.obs")
diag(corr_pcomp_test) <- 0.0

threshold <- 0.05
seeds.test_impute <- c(9876, 6543, 3210, 1999)

for(s in seeds.test_impute) {
    test_imputed <- test
    set.seed(s)
    
    # Imputation of _numeric_ variables:
    for( i in (idx_numeric-1) ) {
        yname <- colnames(test)[i]
        check <- sum(abs(corr_pcomp_test[yname, ]) > threshold)
    
        if(check == 0) {
            test_imputed[, i] <- impute_with_random_sampling(data = test[, i])
        } else {
            test_imputed[, i] <- impute_with_lm(data = test, 
                                                corr_matrix = corr_pcomp_test, 
                                                i = i, 
                                                threshold = threshold, 
                                                verbose = FALSE)
        }
    }
    # Fixing _non-numeric_ (i.e. factor) variables:
    for( i in (idx_factors-1) ) {
        test_imputed[, i] <- impute_with_random_sampling(data = test[, i])
    }

    assign(paste0("test_imputed_", s), test_imputed)
}
```

```{r impute_test-check_imputation, echo = FALSE, eval = FALSE}
# We can verify if there are any variables still missing some data points:
table(sapply(test_imputed, function(x) sum(is.na(x))), useNA = "always")
```

<hr class="thin_separator">
<a name="MODELING"></a>

# MODELING 

We first tried to model the data with __linear regression__, with very poor results as shown below,
and then with __Multivariate Adaptive Regression Splines__ (MARS, as implemented in the R package `earth`).
This latter yield quite decent _in-sample_ and _out-of-sample_ results.

MARS models have the advantage of being able to automatically model non-linearities and "perform"
variable selection, while remaining fairly interpretable.


## _Training_/_Testing_ sets 

The first step in any modeling is splitting the data into training and testing subsets, and
we opted for a 80/20 split (and set the seed).

```{r split_data, echo = TRUE}
# Set seeds for splitting programmatically from seeds used for imputation.
seeds.train_split <- seeds.train_impute*10 + 1

# For each seed (###) create objects:
#  - split.train_### : indices of 'training' split
#  - spTrain_###     : 'training' data set split
#  - spTest_###      : 'testing' data set split
for( k in 1:length(seeds.train_split) ) {
    allTrain_imputed <- get(paste0("train_imputed_", seeds.train_impute[k]))
    set.seed(seeds.train_split[k])
    
    split.train <- createDataPartition(allTrain_imputed$target, p = 0.8, list = FALSE, times = 1)
    assign(paste0("split.train_", seeds.train_impute[k]), split.train)

    assign(paste0("spTrain_", seeds.train_impute[k]), allTrain_imputed[split.train, ])
    assign(paste0("spTest_",  seeds.train_impute[k]), allTrain_imputed[-split.train, ])
}
```

<hr class="thin_separator">
<a name="linear_regression"></a>

## Linear Regression

We built a model in successive steps: 

* We started with using all variables and then 
* reduced the set of predictors to only those that were deemed significant by the previous model, two times in a row.
* The last fit is done adding to the significant variables of the third model the 6 variables that
   show some correlation with the `target`.

Here we report only the results of this last fourth step, and note that it did not actually
represent a major improvement over the most basic `target` vs. _all_ first model. 
A full summary of the model is included in the [Appendix](#Appendix_LM).

```{r set_split_files_for_plots, echo = FALSE}
# loads saved model
load(file = "my_data/lm4.RData")

# shortcut for later use
spTrain <- spTrain_6464
spTest <- spTest_6464
```

### Performance 

The summary performance metrics are pretty poor, namely:

* On the training set itself, _i.e._ in-sample: 
    * __MSE__ = __`r round(lm4.train_metrics$MSE, 2)`__
    * __$R^2$__ = __`r round(lm4.train_metrics$R2, 4)`__
* On the testing set, _i.e._ out-of-sample: 
    * __MSE__ = __`r round(lm4.test_metrics$MSE, 2)`__
    * __$R^2$__ = __`r round(lm4.test_metrics$R2, 4)`__


### Diagnostics Plots

The following plots show the relationship between predicted and observed values of `target` as
scatter and residuals plots, as well as the difference between the distribution of observed and
predicted values.   
About these latter it is particularly clear from its _wavy_ shape that both in-sample and out-of-sample
the model predicts a much narrower distribution of values than the observed ones.

<button class="toggle_plot_code">show plot code</button>
```{r LM-diagnostic_plots, fig.width = 9, fig.height = 6.5, echo = TRUE}
par(fig = c(0, 1, 0, 1))
single_panel_mar <- c(3, 2, 2, 1)
single_panel_oma <- c(1 ,0, 1, 0)
gr_par <- list( mar = single_panel_mar, oma = single_panel_oma, 
                cex = 1.0, cex.axis = 0.8, cex.lab = 1.0, cex.main = 0.9,
                las = 0, mgp = c(1.0, 0.0, 0),
                tcl = 0.3)

mat.layout <- matrix(1:6, nrow = 2, byrow = TRUE)
layout(mat.layout)
par(gr_par)

# Training set : predicted vs. observed
plot_data_vs_prediction(spTrain$target, lm4.predict_train, xlim = 25, ylim = 25, main = "(lm4) In-Sample: ")

# Training set : residuals
plot_data_vs_prediction(spTrain$target, residuals(lm4.model), type = "r", xlim = 25, ylim = 20, main = "(lm4) In-Sample: ")

# Training set : difference between histograms
plot_hist_difference(spTrain$target, lm4.predict_train)

# Testing set : predicted vs. observed
plot_data_vs_prediction(spTest$target, lm4.predict_test, xlim = 25, ylim = 25, color = c(0, 0.5, 0), main = "(lm4) Out-of-Sample: ")

# Testing set : residuals
plot_data_vs_prediction(spTest$target, (spTest$target - lm4.predict_test), type = "r", xlim = 25, ylim = 20, color = c(0, 0.5, 0), main = "(lm4) Out-of-Sample: ")

# Testing set : difference between histograms
plot_hist_difference(spTest$target, lm4.predict_test, color = "green4")

par(fig = c(0, 1, 0, 1), mar = c(5, 4, 4, 1)+0.1, cex.axis = 1.0)
title("Diagnostic Plots for Linear Regression Model, In-Sample and Out-of-Sample", cex.main = 1.3, outer = TRUE, line = 0.0)
```


<hr class="thin_separator">
<a name="MARS"></a>

## _Multivariate Adaptive Regression Splines_ (MARS)

For MARS we used the implementation part of the `earth` package.
It can be run with built-in __K-Fold Cross Validation__ and in our run we set the number of _folds_ to 5 
and the number of _cross-validations_ per fold to 6.  

At first we used the training/testing 80/20 split described above, but after seeing that the performance
of the model was quite good and "stable" (_e.g._ with respect to variable selection) we performed
the last runs using the entire _Training_ data set, confiding in the extensive built-in _Cross
Validation_ mitigates the risk of over-fitting.

### General Procedure

To smooth-out the effects of imputation and of the stochastic nature of _cross validation_, 
we fit a model for each of the different imputed data sets (in each case with the 5-fold $\times$
6 _cross validations_), and combine their predictions a posteriori.
We allow the model to consider second degree interactions.


### Example Model

For illustration, we show only the results of one of the runs (and loaded from previously saved run).
However, in the next box we show the code that would run one model on each of the imputed
_Training_ data sets.  It is set to `eval = FALSE` for speed of compilation of the document, but
if "activated" it would reproduce the results.

```{r MARS-run_model, eval = FALSE}
# Set seeds for splitting programmatically from seeds used for imputation.
seeds.train_model_MARS <- seeds.train_split*10 + 1

for( k in 1:length(seeds.train_model_MARS) ) {
    split.train <- get(paste0("split.train_", seeds.train_impute[k]))
    allTrain <- get(paste0("train_imputed_", seeds.train_impute[k]))
    
    spTrain <- allTrain[split.train, ]
    spTest <- allTrain[-split.train, ]

    set.seed(seeds.train_model_MARS[k])
    mars.model <- earth(target ~ ., data = allTrain, degree = 2, nfold = 5, ncross = 6, keepxy = TRUE)
    assign(paste0("marsN", k, ".model"), mars.model)
}
```

```{r MARS-load_saved_models, echo = FALSE}
# Instead of running the above chunk, fetch saved models
load(file = "my_data/marsN1.RData")
load(file = "my_data/marsN2.RData")
load(file = "my_data/marsN3.RData")
load(file = "my_data/marsN4.RData")

# for later use
allTrain <- train_imputed_6464
```

#### Model Summary

Please note that MARS models use as basis constants and _hinge functions_, represented in
the summary by the _h(x - value)_ expressions.

```{r MARS-summary-N4}
summary(marsN4.model)
```

#### Important Variables

These are the important (_i.e._ selected/used) variables for this particular model:

<button class="toggle_code">show code</button>
```{r MARS-variable_importance-N4, echo = TRUE}
ev <- evimp(marsN4.model)
tmp.df <- data.frame(nsubsets = ev[, 3], gcv = ev[, 4], rss = ev[, 6])
row.names(tmp.df) <- sprintf('**%s**', row.names(tmp.df))
```

```{r MARS-variable_importance-N4_kable, echo = FALSE}
kable(tmp.df, digits = 2, align = c("c", "r", "r"))
```

### Performance of MARS Models

<button class="toggle_code">show example code</button>
```{r MARS-predict_train-N4, eval = FALSE, echo = TRUE}
marsN4.predict_train <- predict(marsN4.model, allTrain[, -1])
marsN4.metrics <- compute_metrics(data = allTrain$target, prediction = marsN4.predict_train)
```

```{r MARS-prepare_metrics_table, echo = FALSE}
mars.perf.summary <- data.frame(MARS1 = unlist(marsN1.metrics), 
                                MARS2 = unlist(marsN2.metrics), 
                                MARS3 = unlist(marsN3.metrics),
                                MARS4 = unlist(marsN4.metrics))
```

The _MSE_ and $R^2$ metrics for the four final MARS models are summarized in this table:

```{r MARS-metrics_table, echo = FALSE}
kable(mars.perf.summary, digits = 4, caption = "Summary of MARS Models Performance", align = "c")
```

<button class="toggle_code">show code</button>
```{r MARS-get_used_predictors, echo = TRUE}
marsN1.imp_var <- get_used_pred_names(model = marsN1.model) %>% gsub("(f_[0-9]+).*$", "\\1", ., perl = TRUE) %>% base::unique(.)
marsN2.imp_var <- get_used_pred_names(model = marsN2.model) %>% gsub("(f_[0-9]+).*$", "\\1", ., perl = TRUE) %>% base::unique(.)
marsN3.imp_var <- get_used_pred_names(model = marsN3.model) %>% gsub("(f_[0-9]+).*$", "\\1", ., perl = TRUE) %>% base::unique(.)
marsN4.imp_var <- get_used_pred_names(model = marsN4.model) %>% gsub("(f_[0-9]+).*$", "\\1", ., perl = TRUE) %>% base::unique(.)
marsNall.imp_var <- c(marsN1.imp_var, marsN2.imp_var, marsN3.imp_var, marsN4.imp_var) %>% base::unique(.) %>% 
                      gsub("f_", "", .) %>% as.integer(.) %>% sort(.) %>% paste0("f_", .)
```

They are all equivalent, also from the point of view of _variable selection_ where the four
models only differ on 1-2 of the lowest importance variables.   
The combined list of important variables is:

```{r MARS-print_combined_important_variables}
marsNall.imp_var
```

Note that some of them are _factors_:  __`r marsNall.imp_var[sapply(marsNall.imp_var, function(x) is.character(train[, x]))]`__.


### Diagnostics Plots

For direct comparison with the _linear regression_ results we plot here the scatter and residuals plots,
and the difference between observed and predicted distributions of `target` (for one of the _MARS_ models).   
There still remains a systematic slant to the residuals, but it is acceptable, and the difference
between the distributions is more even than before.

<button class="toggle_plot_code">show plot code</button>
```{r MARS-diagnostic_plots-N4, fig.width = 9, fig.height = 3.5, echo = TRUE}
par(fig = c(0, 1, 0, 1))
single_panel_mar <- c(3, 2, 2, 1)
single_panel_oma <- c(1 ,0, 1, 0)
gr_par <- list( mar = single_panel_mar, oma = single_panel_oma, 
                cex = 1.0, cex.axis = 0.8, cex.lab = 1.0, cex.main = 0.9,
                las = 0, mgp = c(1.0, 0.0, 0),
                tcl = 0.3)

mat.layout <- matrix(1:3, nrow = 1, byrow = TRUE)
layout(mat.layout)
par(gr_par)

# Training set : predicted vs. observed
plot_data_vs_prediction(allTrain$target, marsN4.predict_train, xlim = 25, ylim = 25, color = c(0, 0, 0.8), main = "(marsN4): ")

# Training set : residuals
plot_data_vs_prediction(allTrain$target, residuals(marsN4.model), type = "r", xlim = 25, ylim = 20, color = c(0, 0, 0.8), main = "(marsN4): ")

# Training set : difference between histograms
plot_hist_difference(allTrain$target, marsN4.predict_train, color = "blue2")

par(fig = c(0, 1, 0, 1), mar = c(5, 4, 4, 1)+0.1, cex.axis = 1.0)
title("Diagnostic Plots for MARS Model, In-Sample and Out-of-Sample", cex.main = 1.3, outer = TRUE, line = 0.0)
```


<hr class="thin_separator">
<a name="Prediction"></a>

## Predicting the _Testing_ Data Set

<button class="toggle_code">show code</button>
```{r MARS-predict_test_set-ALL, cache = TRUE, echo = TRUE}
# For each model (i) predict each imputed data set (k).
#  - NOTE: model 'i' was trained on data set 'k=i'.
tmp_summary_list <- list()
i_max <- length(seeds.train_impute)
k_max <- length(seeds.test_impute)
for( i in 1:i_max ) {
    name.model <- paste0("marsN", i, ".model")
    
    tmp_df <- list()
    for( k in 1:k_max ) {
        name.prediction <- paste0("predict_test_N", k)
        name.test_data  <- paste0("test_imputed_", seeds.test_impute[k])
        tmp_df[[k]] <- predict(get(name.model), newdata = get(name.test_data))
    }
    tmp_df <- as.data.frame(tmp_df)
    colnames(tmp_df) <- c(paste0("test", 1:k_max))
    tmp_df$mean <- apply(tmp_df[, 1:k_max], 1, function(x) (mean(x)))
    tmp_df$sd   <- apply(tmp_df[, 1:k_max], 1, function(x) (sd(x)))
    tmp_summary_list[[i]] <- tmp_df[, (k_max+1):(k_max+2)]
}
summaryNall <- do.call(bind_cols, tmp_summary_list)
colnames(summaryNall) <- c(paste0(rep(c("mean", "sd"), i_max), rep(1:i_max, each = 2)))
rm(tmp_summary_list, tmp_df)
```

<button class="toggle_code">show code</button>
```{r MARS-merge_prediction_on_test_sets, cache = TRUE, echo = TRUE}
# Adding more "diagnostic" variables 
idx_col_mean <- (1:ncol(summaryNall))[grepl("^mean", colnames(summaryNall))]
idx_col_sd <- (1:ncol(summaryNall))[grepl("^sd", colnames(summaryNall))]

summaryNall$NzeroVar   <- apply(summaryNall[, idx_col_sd],   1, function(x) {sum( x == 0 )} )
summaryNall$target     <- apply(summaryNall[, idx_col_mean], 1, function(x) {mean(x)} )
summaryNall$delta      <- apply(summaryNall[, idx_col_mean], 1, function(x) {max(x) - min(x)} )
summaryNall$deltaNorm  <- apply(summaryNall[, idx_col_mean], 1, function(x) {(max(x) - min(x))/abs(mean(x))} )
summaryNall$deltaNorm2 <- apply(summaryNall[, idx_col_mean], 1, function(x) {(max(x) - min(x))/(max(x) + min(x))} )
summaryNall$mse        <- apply(summaryNall[, idx_col_mean], 1, function(x) {0.25*sum((x-mean(x))^2)} )
```

We used each of the four models (coming from the four imputed _Training_ data sets) to predict
`target` for each of the four imputed _Testing_ data sets.
__For each model we took as its prediction the average of `target` values over the four imputed _Testing_ sets__.


### Quick _Health_ Checks of Predictions

To assess the effect of imputation, we compared the variability of its predictions on the four
imputed _Testing_ data sets, computing mean and standard deviation.

For each observation we have then four _mean_ and _sd_, one for each model over the four imputed _Testing_ sets.
We can look at the amount of variation from model to model, for instance by inspecting the
_max-min_ range of predicted values for each observation, or the _"MSE"_ by observation.
We look at them in the next two plots.

<button class="toggle_plot_code">show plot code</button>
```{r MARS-plots-predicted_variables_minmax_and_MSE, fig.width = 10, fig.height = 3.5, echo = TRUE, cache = FALSE}
mat.layout <- matrix(1:2, nrow = 1, byrow = TRUE)
layout(mat.layout)

options(scipen = 1)
single_panel_mar <- c(3, 3, 1, 1)
single_panel_oma <- c(0 ,0, 2, 0)
gr_par <- list( mar = single_panel_mar, oma = single_panel_oma, 
                cex = 1.0, cex.axis = 1.0, cex.lab = 1.2, cex.main = 1.0, 
                las = 0, mgp = c(1.5, 0.3, 0),
                tcl = 0.3)
par(gr_par)

plot(summaryNall$deltaNorm, pch = 20, cex = 1.25, col = rgb(0.0, 0.0, 0.8, 0.3), 
     ylim = c(0.003, 100), 
     xlab = "index", 
     ylab = "max - min", 
     main = "(Max - Min) of Predicted Values", 
     log = "y", yaxt = "n")
y1 <- floor(log10(range(summaryNall$delta)))
pow <- seq(y1[1]-1, y1[2]+1)
ticksat <- as.vector(sapply(pow, function(p) (1:10)*10^p))
axis(2, 10^pow)
axis(2, ticksat, labels = NA, tcl = 0.5, lwd = 0, lwd.ticks = 1, las = 1)
grid()

plot(summaryNall$mse, pch = 20, cex = 1.25, col = rgb(0.0, 0.0, 0.8, 0.3), 
     ylim = c(0.00005, 7), 
     xlab = "index", 
     ylab = "MSE", 
     main = "MSE of Predicted Values", 
     log = "y", yaxt = "n")
y1 <- floor(log10(range(summaryNall$delta)))
pow <- seq(y1[1]-1, y1[2]+1)
ticksat <- as.vector(sapply(pow, function(p) (1:10)*10^p))
axis(2, 10^pow)
axis(2, ticksat, labels = NA, tcl = 0.5, lwd = 0, lwd.ticks = 1, las = 1)
abline(h = 0.01, lty = 2, col = "red2", lwd = 2)
grid()

par(fig = c(0, 1, 0, 1), mar = c(5, 4, 4, 1)+0.1, cex.axis = 1.0)
title("Some Check on Predicted Values", cex.main = 1.1, outer = TRUE, line = 0.5)
```

In both quantities there are only a few cases at large deviations; for instance, as a reference,
there are `r nrow(subset(summaryNall, mse > 0.01))` with $MSE > 0.01$ (marked by the red dashed
line).

Overall the predictions seem to be quite robust, from the point of view of the potential
variations caused by imputation and variable selection.


### Prediction

As noted, our final prediction for the _Testing_ data set is the _grand mean_ of the predictions
of the four models "crossed" with the four imputed _Testing_ data.

```{r write_prediction}
write.csv(summaryNall$target, file = "data/prediction.csv", row.names = FALSE, quote = FALSE)
```


<hr class="thin_separator">
<a name="FINAL_THOUGHTS"></a>

## FINAL THOUGHTS

[[Back to the Top](#TOP)]

#### Imputation 

The issue of missing data required to impute them before to proceed with modeling as they
were scattered around the data set in such a way that most models would not have been applicable,
or we would have incurred a high "cost" in terms of loss of usable training data.

Based on a quick coarse review of the data, we made the assumption that the data were missing
completely at random, and so it was acceptable to impute them with a simple approach.
Given the small fraction of missingness for each variable (and observation), imputation by random
sampling from the existing data is likely good enough.  Nevertheless, for the few variables showing
some degree of correlation with other, we included the possibility of imputing their missing
values via linear regression on the correlated predictors.

#### Best model

The model we adopted is _Multivariate Adaptive Regression Splines_ (MARS), because it offers
an attractive combination of light computational "cost", flexibility, built-in variable selection, 
interpretatibility.   In the final runs we trained models on the entire data set, relying on
_K-fold Cross Validation_ to avoid over-fitting.  We obtained several models by fitting
on different versions of the imputed data set.
On the _Training_ data sets these models performance is quite good, with $MSE \simeq 2.4$ 
and $R^2 \simeq 0.91$.

We computed predictions of four models on each of four imputed _Testing_ data sets, and then averaged
their predictions.   This average is what we saved and submitted.

The quick checks that we did on the variations on the predictions on the _Testing_ data set seem
to suggest that the models are quite "stable", in the sense that only a few observations exhibit
significant variance in the predictions.
However, this is more of a measure of the effect of imputation and of the small differences
between models (important variables, coefficients), and while it is comforting to see that 
we get consistent results, this may not translate in good performance on the actual `target` values 
of the _Testing_ data set.

#### Concerns 

One reason of concern is that the statistical properties of the data seem to be slightly
different between the _Training_ and the _Testing_ data set, in particular the apparent broader 
range of values spanned by _quantiles_ (and in turn _IQR_).   

On the other hand, considering that the model is using only about a dozen variables, it is possible
that its performance may not be so seriously affected by this apparent difference.
To get a glimpse on this, we repeat below some of the plots of the [earlier summary section](#summary_numeric),
marking with blue circles the _important variables_.
Qualitative speaking one could argue that the blue symbols are less spread than the full set of variables.
However, with respect to generalization of the model the truly ideal finding would be that the important variables 
were clusted closely around the diagonal, _i.e._ with identical characteristics in the _Training_
and _Testing_ data sets, at least as far as these summaries can go.


<button class="toggle_plot_code">show plot code</button>
```{r plot-predictors_summary_stats_train_vs_test_with_imp_vars, fig.width = 9, fig.height = 3.5, echo = TRUE}
par(fig = c(0, 1, 0, 1))
single_panel_mar <- c(2, 2, 2, 1)
single_panel_oma <- c(0 ,0, 2, 0)
gr_par <- list( mar = single_panel_mar, oma = single_panel_oma, 
                cex = 1.0, cex.axis = 0.8, cex.lab = 1.0, 
                las = 0, mgp = c(1.0, 0.0, 0),
                tcl = 0.3)

mat.layout <- matrix(1:3, nrow = 1, byrow = TRUE)
layout(mat.layout)

test_flag.imp_var <- (colnames(train) %in% marsNall.imp_var)[idx_numeric]

par(gr_par)
plot_stats_vs_stats(data1 = train.stats_by_column, data2 = test.stats_by_column, var = "q025", flag = test_flag.imp_var)
plot_stats_vs_stats(data1 = train.stats_by_column, data2 = test.stats_by_column, var = "q075", flag = test_flag.imp_var)
plot_stats_vs_stats(data1 = train.stats_by_column, data2 = test.stats_by_column, var = "IQR", flag = test_flag.imp_var)
    
par(fig = c(0, 1, 0, 1), mar = c(5, 4, 4, 1)+0.1, cex.axis = 1.0)
title("Comparison of Summary Statistics of Numeric Variables", cex.main = 1.1, outer = TRUE, line = 1)
title("(with Important Variables Highlighted)", cex.main = 1.1, outer = TRUE, line = 0)
```


<hr class="thin_separator">
<a name="APPENDIX"></a>

# APPENDIX

[[Back to the Top](#TOP)]

<a name="Appendix_NA"></a>

## Statistics of `NA` 

### _Training_ Data Set

#### By Variable

```{r stats_NA_in_train-columns_v2, echo = FALSE}
tl <- kable_a_table_wrapped(train.NA_by_column, width = 16)
kable(tl[[1]], caption = "Contingency Table of NA Counts in Columns")
kable(tl[[2]])
kable(tl[[3]])
```

#### By Observation

```{r stats_NA_in_train-rows, echo = FALSE}
# table(train.NA_by_row)
kable_a_table(train.NA_by_row, caption = "Contingency Table of NA Counts in Rows")
```

### _Testing_ Data Set

#### By Variable

```{r stats_NA_in_test-columns, echo = FALSE}
# kable_a_table(test.NA_by_column, caption = "Contingency Table of NA Counts in Columns")
tl <- kable_a_table_wrapped(test.NA_by_column, width = 12)
kable(tl[[1]], caption = "Contingency Table of NA Counts in Columns")
kable(tl[[2]])
```

#### By Observation

```{r stats_NA_in_test-rows, cache = FALSE, echo = FALSE}
kable_a_table(test.NA_by_row, caption = "Contingency Table of NA Counts in Rows")
```


<hr class="thin_separator">
<a name="Appendix_LM"></a>

## Linear Regression Model Summary (Fit #4)

Setup of variables and model formula:
```{r LM4-setup}
varnames_lm3_0005 <- c("(Intercept)", "f_35", "f_54", "f_61", "f_94", "f_175", 
                                      "f_196", "f_205", "f_217", "f_218", "f_237")
var_from_corr <- c("f_35", "f_47", "f_161", "f_175", "f_195", "f_205", "f_218")
lm4.vars <- c(varnames_lm3_0005, var_from_corr) %>% base::unique(.)
lm4.formula <- paste("target", paste0(lm4.vars[-1], collapse = " + "), sep = " ~ ")
```

Model fit, and summary:

<button class="toggle_code">show code and output</button>
```{r LM4-run_model}
spTrain <- spTrain_6464
lm4.model <- lm(as.formula(lm4.formula), data = spTrain)

summary(lm4.model)
```

```{r LM4-predict_train, echo = FALSE}
# Predict on train and compute some basic metrics (_i.e._ _MSE_ and $R^2$):
lm4.predict_train <- predict(lm4.model)
lm4.train_metrics <- compute_metrics(data = spTrain$target, prediction = lm4.predict_train)
```

```{r LM4-predict_test, echo = FALSE}
# Predict on test and compute some basic metrics (_i.e._ _MSE_ and $R^2$):
lm4.predict_test <- predict(lm4.model, newdata = spTest)
lm4.test_metrics <- compute_metrics(data = spTest$target, prediction = lm4.predict_test)
```

<hr class="thin_separator">
<a name="Appendix_MARS"></a>

## MARS Models Summary

#### Summaries

<button class="toggle_code">show code and output</button>
```{r MARS-model_summary-ALL}
for( i in 1:length(seeds.train_impute) ) {
    print( summary(get(paste0("marsN", i, ".model"))) )
    cat("\n--------------------------------------------------------------------------------\n\n")
}
```

#### Important Variables

Lists for each of the four _MARS_ fits.

<button class="toggle_code">show code</button>
```{r MARS-variable_importance-ALL, echo = TRUE, results='asis'}
for( i in 1:length(seeds.train_impute) ) {
    ev <- evimp(marsN4.model)
    tmp.df <- data.frame(nsubsets = ev[, 3], gcv = ev[, 4], rss = ev[, 6])
    row.names(tmp.df) <- sprintf('**%s**', row.names(tmp.df))
    cat("\n* __MARS Model ", i, "__\n")
    print( kable(tmp.df, digits = 2, align = c("c", "r", "r"), 
                 format = "markdown",
                 caption = paste0("'MARS Model ", i, "'") )
    )
}
```

<hr class="thin_separator">
<a name="APPENDIX_user_functions"></a>

## User defined functions

[[Back to the Top](#TOP)]

```{r echo = FALSE, cache = FALSE}
read_chunk("./scripts/my_functions_miscellaneous.R")
read_chunk("./scripts/my_functions_modeling.R")
read_chunk("./scripts/my_functions_plot.R")
```

Additional locally defined functions, sourced from external files.

<ul>
<li>
__Miscellaneous__ / [source on GitHub](https://github.com/pedrosan/DataScienceExamples/blob/master/Synthetic_Data/scripts/my_functions_miscellaneous.R) / 
<button class="toggle_code">show code</button>
```{r eval = FALSE, cache = FALSE, echo = TRUE}
<<my_functions_misc>>
```
</li>

<li>
__Modeling__ / [source on GitHub](https://github.com/pedrosan/DataScienceExamples/blob/master/Synthetic_Data/scripts/my_functions_modeling.R) / 
<button class="toggle_code">show code</button>
```{r eval = FALSE, cache = FALSE, echo = TRUE}
<<my_functions_modeling>>
```
</li>

<li>
__Plotting__ / [source on GitHub](https://github.com/pedrosan/DataScienceExamples/blob/master/Synthetic_Data/scripts/my_functions_plot.R) / 
<button class="toggle_code">show code</button>
```{r eval = FALSE, cache = FALSE, echo = TRUE}
<<my_functions_plot>>
```
</li>
</ul>


<hr class="thin_separator">
<a name="SessionInfo"></a>

## R Session Info

```{r R_session_info}
sessionInfo()
```
---
