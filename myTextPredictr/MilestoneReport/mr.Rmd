---
title: "Building a Text Prediction Algorithm"
subtitle    : "Exploratory Analysis and Thoughts about a Prediction Strategy"
author      : Giovanni Fossati
job         : 
output      : 
  html_document:
    self_contained: false
---

```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
require("knitr")
options(width = 100, 
        scipen = 5)
opts_chunk$set(message = FALSE, 
               error = FALSE, 
               warning = FALSE, 
               collapse = TRUE, 
               tidy = FALSE,
               cache = FALSE, 
               cache.path = '.cache/', 
               comment = '#',
               fig.align = 'center', 
               dpi = 100, 
               dev = "png",
               fig.path = 'figures/')
knit_print.tbl_df <- function(x, options) {
  knitr::knit_print(trunc_mat(x), options)
}
```

<a name="SUMMARY"></a>

# SUMMARY

In this report I briefly illustrate the exploratory analysis performed on a three datasets,
comprising text from blogs, news and tweets.

The ultimate goal is to produce a _light_ application able to predict text (words) given some
preceding text, mimicking the predictive typing feature of modern software keyboard of portable
devices.

As a playground a fairly substantial dataset was made available, comprising text from various
heterogenous sources (blogs, news, twitter). 
These datasets are the foundation for developing an understanding of _language processing_ 
and in turn devise a strategy for achieving the goal, and perhaps more importantly (in practice) 
they constitute our training and testing datasets.

I decided to invest a significant amount of time to explore the data, and delved (too) deeply into
data cleaning, assuming that this effort will pay off by making any algorithm more robust.

At this stage in the project I will mostly review my exploratory analysis of the data, 
and outline my current thought about the strategy for developing the algorithm for the
text-predicting application.

__Performance issues__: it is worth mentioning that one of the main challenges has been 
dealing smartly with the computational load, that turned out to be a serious limiting factor, 
even on a powerful workstation.  
I did not use the suggested `tm` suite and relied instead heavily on `perl` and in `R` mainly `dplyr`, `NLP` and `RWeka`.


### Current Thoughts About Predictive Algorithm Strategy 

My current thoughts, very much in flux, about the strategy are that a _n-grams_ based approach
would be the most effective.  
In particular, I am leaning towards a _weighted combination_ of _2- 3- 4- 5-grams_ (linear
interpolation), perhaps _assisted_ by some additional information drawn from an analysis
of the association of words in sentences or their distance within it.

An important issue that I have not yet had a chance to ponder sufficiently include the handling of
_"zeros"_, _i.e._ words not included in the dictionary of the training set or, more importantly
with a _n-grams_ approach words that are not seen following a given _(n-1) gram_.
In practice, based on my readings, this problem is tackled with some form of _smoothing_, 
that is assigning a probability to the _"zeros"_ (and in turn re-allocating some mass probability
away from the observed _n-grams_).  
I have not yet had a chance to explore the feasibility and effectiveness of methods
like _Good-Turing_ or _Stupid Backoff_.


<a name="CONTENT"></a>

## CONTENT

The report is organized in the following sections:

* [EXECUTIVE SUMMARY](#SUMMARY)
* [PRELIMINARIES](#PRELIMINARIES)
    * [Preprocessing (before loading into R)](#PREPROCESSING)
        * [Removal of weird characters](#weird_characters)
        * [Homogeneization of characters](#Homogeneization)
        * [Contractions, profanities, emoticons, hashtags, etc...](#Contractions)
        * [Excluding rows with less than 6 words](#exclude_short_rows)
* [MOVING TO R](#LOADING)
    * [Loading the Data](#LOADING_THE_DATA)
    * [Further Data Cleaning in R](#FURTHER_CLEANING) 
        * [Text transformations](#text_transformations)
        * [Subsetting of the data](#subsetting)
* [ANALYSIS](#ANALYSIS)
    * [Step 1 : Sentence Annotation](#ANALYSIS-STEP1)
    * [Step 2 : _n-grams_ Tokenization](#ANALYSIS-STEP2)
        * [Cleaning and _re-classification_ of n-grams](#ngrams-cleaning_and_reclassification)
        * [A look at the n-grams](#ngrams_tables)
        * [Some Summary Plots for 4-grams](#ngrams_plots)
            * [Top-30 all mixed](#top30_mixed)
            * [Top-20 by data source](#top20_separate)
* [APPENDIX](#APPENDIX)
    * [User Defined Functions](#APPENDIX-my_functions)
    * [Summary of regularization done with _perl scripts_](#APPENDIX-regularize)
    * [Summary of post-sentence-tokenization cleaning (with _perl scripts_)](#APPENDIX-post_sentence_cleaning)
    * [Mysterious issue with `NGramTokenizer`](#APPENDIX-tokenization_issue)


<hr class="thin_separator">
<a name="PRELIMINARIES"></a>

# PRELIMINARIES

[Back to the Top](#TOP)

Libraries needed for data processing and plotting:

<button class="toggle_code">show code</button>
```{r load_packages, cache = FALSE, echo = TRUE, message = FALSE, warning = FALSE, tidy = FALSE}
#-----------------------------
# NLP
library("tm")
library("SnowballC")
library("openNLP")
library("NLP")

# To help java fail less :-(
options( java.parameters = "-Xmx6g")
library("RWeka")   # [NGramTokenizer], [Weka_control]

#-----------------------------
# general
library("dplyr")
library("magrittr")
library("devtools")

library("ggplot2")
library("gridExtra")
# library("RColorBrewer")

library("pander")

#-----------------------------
# my functions
source("./scripts/my_functions.R")
#-----------------------------
```

## THE DATA

[Back to TOC](#CONTENT)

The datasets are read-in separately into character vectors, using a user-defined compact function
(`readByLine()`) (see [__Appendix__](#APPENDIX-my_functions) for the short source).

```{r load_data, eval = FALSE, cache = TRUE}
# NOT EVALUATED because too computationally heavy (loading saved products)
in.blogs.ORIG <- readByLine("./data/en_US.blogs.ORIGINAL.txt.gz", check_nl = FALSE, skipNul = TRUE)
in.news.ORIG <- readByLine("./data/en_US.news.ORIGINAL.txt.gz", check_nl = FALSE, skipNul = TRUE)
in.twitter.ORIG <- readByLine("./data/en_US.twitter.ORIGINAL.txt.gz", check_nl = FALSE, skipNul = TRUE)
```

Basic statistics of the three datasets in their original form:

<button class="toggle_code">show code</button>
```{r data-basic_stats, eval = FALSE, cache = TRUE, echo = TRUE}
# NOT EVALUATED
stats.blogs   <- as.numeric(system("gzip -dc ./data/en_US.blogs.ORIGINAL.txt.gz | wc | awk '{print $1; print $2; print $3}'", intern = TRUE))
stats.news    <- as.numeric(system("gzip -dc ./data/en_US.news.ORIGINAL.txt.gz | wc | awk '{print $1; print $2; print $3}'", intern = TRUE))
stats.twitter <- as.numeric(system("gzip -dc ./data/en_US.twitter.ORIGINAL.txt.gz | wc | awk '{print $1; print $2; print $3}'", intern = TRUE))

stats.ORIG.df <- data.frame( blogs = stats.blogs, news = stats.news, twitter = stats.twitter, 
                             row.names = c("lines", "words", "characters"), stringsAsFactors = FALSE)
saveRDS(stats.ORIG.df, "data/stats_ORIGINAL.RDS")
```

```{r data-basic_stats-read, eval = TRUE, echo = FALSE}
stats.ORIG.df <- readRDS("data/stats_ORIGINAL.RDS")
```

```{r data-basic_stats-kable, eval = TRUE, echo = FALSE}
kable(stats.ORIG.df)
```


<a name="PREPROCESSING"></a>

## PREPROCESSING (before loading into R)

[Back to TOC](#CONTENT)

After a quick review of the data with various R functions and packages, 
I decided to perform some cleaning of the text with standard _Linux_ command line tools,
mostly __perl__ scripts.
Broadly speaking I performed three categories of transformations:

* Removal of "weird" characters.
* Homogeneization of characters.
* Regularization of text, _e.g._ dealing with hashtags, profanities, emoticons, acronyms, numbers, "messy punctuation"

which I will describe below.


<a name="weird_characters"></a>

### _Weird_ Characters

The first task was to analyze the mix of invidual characters present in the three datasets
with the goal of doing some homogeneization and tidying up of non-alphanumeric characters,
such as quotes that can come in different forms.

The used method is not elegant, but effective enough, relying on a simple perl command
substituting a series of _non-odd_ characters with spaces, thus leaving a stream of _odd_ characters
subsequently parsed and cleaned to produce a list of _odd_ characters sorted by their count.

```{r perl_example, engine = "bash", eval = FALSE}
perl -pe 's|[\d\w\$\,\.\!\?\(\);:\/\\\-=&%#_\~<>]||g; s|\s||g; s|[\^@"\+\*\[\]]||g;' | \
          perl -pe "s/\'//g;" | \
          egrep -v '^$' | \
          split_to_singles.awk | \
          sort -k 1 | uniq -c | sort -k 1nr

# split_to_singles.awk is a short awk script not worth including here (it's on GitHub)
```

The number of unique _odd_ characters found in each dataset are 
`r system("wc -l data/odd_chars.sorted.blogs.txt | awk '{print $1}'", intern = TRUE)` for blogs, 
`r system("wc -l data/odd_chars.sorted.news.txt | awk '{print $1}'", intern = TRUE)` for news,
`r system("wc -l data/odd_chars.sorted.twitter.txt | awk '{print $1}'", intern = TRUE)` for twitter.

The following is the census of _odd characters_ appearing more than 500 times in each of the datasets 
(the full sorted lists are available on the GitHub repo in the data directory).

```{r dummy_1, echo = TRUE, eval = FALSE}
   blogs           news              twitter
-----------      ----------         ------------------------
 387317 [‚Äô]      102911 [‚Äô]         27440 [‚Äú]        726 [¬ª]
 109154 [‚Äù]       48115 [‚Äî]         26895 [‚Äù]        718 [¬´]
 108769 [‚Äú]       47090 [‚Äú]         11419 [‚Äô]        715 [üòî]
  50176 [‚Äì]       43992 [‚Äù]          5746 [‚ô•]        686 [üòâ]
  41129 [‚Ä¶]        8650 [‚Äì]          5241 [‚Ä¶]        680 [üò≥]
  23836 [‚Äò]        6991 [√∏]          3838 [|]        639 [{]
  18757 [‚Äî]        6723 [¬ì]          2353 [‚ù§]        617 [‚Ä¢]
   3963 [√©]        6544 [¬î]          2314 [‚Äì]        593 [‚Äò]
   2668 [¬£]        6267 [¬í]          1799 [‚Äî]        578 [ÔøΩ]
   1301 [‚Ä≤]        4898 [‚Äò]          1333 [üòä]        561 [üíú]
    914 [¬¥]        3641 [¬ñ]          1211 [üëç]        560 [üòÉ]
    755 [‚Ä≥]        3319 [√©]          1149 [üòÇ]        544 [üòè]
    643 [‚Ç¨]        3062 [‚Ä¶]           977 [√©]        506 [‚òÄ]
    624 [ƒÅ]        2056 [¬ó]           963 [üòÅ]        503 [üòú]
    605 [¬Ω]        1408 [¬ï]           955 [‚ò∫]
    598 [√°]        1152 [ÔøΩ]           926 [üòí]
    582 [√∂]         971 [‚Ä¢]           802 [`]
    555 [√®]         837 [¬Ω]           758 [üòç]
    518 [¬∞]         711 [`]           751 [üòò]
                    537 [√±]           741 [}]
```

```{r summary_table_1, echo = FALSE, eval = FALSE, results = 'asis'}
tab1 <- "
| blogs  |       |   news |        |twitter|      |      |      |
|-------:|:-----:|-------:|:------:|------:|:----:|-----:|:----:|
| 387317 |   ‚Äô   | 102911 |  ‚Äô     | 27440 |   ‚Äú  |  726 |   ¬ª  |
| 109154 |   ‚Äù   |  48115 |  ‚Äî     | 26895 |   ‚Äù  |  718 |   ¬´  |
| 108769 |   ‚Äú   |  47090 |  ‚Äú     | 11419 |   ‚Äô  |  715 |   üòî  |
|  50176 |   ‚Äì   |  43992 |  ‚Äù     |  5746 |   ‚ô•  |  686 |   üòâ  |
|  41129 |   ‚Ä¶   |   8650 |  ‚Äì     |  5241 |   ‚Ä¶  |  680 |   üò≥  |
|  23836 |   ‚Äò   |   6991 |  √∏     |  3838 |   |  |  639 |   {  |
|  18757 |   ‚Äî   |   6723 |  ¬ì  |  2353 |   ‚ù§  |  617 |   ‚Ä¢  |
|   3963 |   √©   |   6544 |  ¬î  |  2314 |   ‚Äì  |  593 |   ‚Äò  |
|   2668 |   ¬£   |   6267 |  ¬í  |  1799 |   ‚Äî  |  578 |   ÔøΩ  |
|   1301 |   ‚Ä≤   |   4898 |  ‚Äò     |  1333 |   üòä  |  561 |   üíú  |
|    914 |   ¬¥   |   3641 |  ¬ñ  |  1211 |   üëç  |  560 |   üòÉ  |
|    755 |   ‚Ä≥   |   3319 |  √©     |  1149 |   üòÇ  |  544 |   üòè  |
|    643 |   ‚Ç¨   |   3062 |  ‚Ä¶     |   977 |   √©  |  506 |   ‚òÄ  |
|    624 |   ƒÅ   |   2056 |  ¬ó  |   963 |   üòÅ  |  503 |   üòú  |
|    605 |   ¬Ω   |   1408 |  ¬ï  |   955 |   ‚ò∫  |      |      |
|    598 |   √°   |   1152 |  ÔøΩ     |   926 |   üòí  |      |      |
|    582 |   √∂   |    971 |  ‚Ä¢     |   802 |   x  |      |      |
|    555 |   √®   |    837 |  ¬Ω     |   758 |   üòç  |      |      |
|    518 |   ¬∞   |    711 |  `     |   751 |   üòò  |      |      |
|        |       |    537 |  √±     |   741 |   }  |      |      |
"
cat(tab1)
```


<a name="Homogeneization"></a>

### Homogeneization of Characters

[Back to TOC](#CONTENT)

For this preliminary stage I decided to not worry about accented letters, and characters from
non-latin alphabet (_e.g._ asian, emoticons), but I thought it would be helpful to standardize
a small set of very frequent characters, whose "meaning" is substantially equivalent

```{r dummy_2, echo = FALSE, eval = FALSE}
                blogs    news  twitter      TOTAL
        
quotes    [‚Äò]   23836    4898     593   =   29327
          [‚Äô]  387317  102911   11419   =  501647
          [‚Äú]  108769   47090   27440   =  183299
          [‚Äù]  109154   43992   26895   =  180041
          [¬´]       0       0     718   =     718
          [¬ª]       0       0     726   =     726
                                
dashes    [‚Äì]   50176    8650    2314   =   61140
          [‚Äî]   48115   18757    1799   =   68671

ellipsis  [‚Ä¶]   41129    5241    3062   =   49432
```

```{r summary_table_2, echo = FALSE, results = 'asis'}
tab2 <- "
|         |     |  blogs |   news | twitter  |   TOTAL |
|:--------|:---:|-------:|-------:|---------:|--------:|
|quotes   | [‚Äò] |  23836 |   4898 |    593   |   29327 |
|         | [‚Äô] | 387317 | 102911 |  11419   |  501647 |
|         | [‚Äú] | 108769 |  47090 |  27440   |  183299 |
|         | [‚Äù] | 109154 |  43992 |  26895   |  180041 |
|         | [¬´] |      0 |      0 |    718   |     718 |
|         | [¬ª] |      0 |      0 |    726   |     726 |
|         |     |        |        |          |         |
|dashes   | [‚Äì] |  50176 |   8650 |   2314   |   61140 |
|         | [‚Äî] |  48115 |  18757 |   1799   |   68671 |
|         |     |        |        |          |         |
|ellipsis | [‚Ä¶] |  41129 |   5241 |   3062   |   49432 |
"
cat(tab2)
```


<a name="Contractions"></a>

### Contractions, Profanities, Emoticons, Hashtags, etc... 

[Back to TOC](#CONTENT)

I have put a major effort into understanding the idiosyncrasies of the textual data, with the
expectation that a deep cleaning would make a difference in the prediction context.

One example of what I have in mind is that transforming to categorical generic "tag" frequent
"items" with a lot of variations but broadly similar meaning (e.g. dates, money, possessive
pronouns), could strengthen the predictive ability of any algorithm.   

Most of the work was done with `perl` "offline" (can't beat it for `regex` work).   
To match the application input with the data on which the application is built, all operations 
were ported to `R` either directly or by relying on an external perl script.
Among the main transformations applied to the text:

* __Contractions__ (_e.g._ don't, isn't, I'll): this seem to be more commonly regarded as 
      stopword, hence removed.  My take has been that they can provide meaning and it was worth 
      preserving them, as well as they non-contracted counterparts.  I homogeneized all 
      of them in forms like "I_will", "do_not", with an underscore gluing them together.
* __Profanity filtering__: I based my cleaning on the "7 dirt words", and some words rooted on them.
    + To preserve their potential predictive value, I replace them with a tag `<PROFANITY>`.
    + User input is also filtered, but the information carried by a possible profanity can be used.
* __Emoticons__: Recognized them with regex.  Marked with a tag, `<EMOJ>`.

Other transformations done on the text before loading the data into R:

- __Regularization/ Homogeneization of Characters__
    - Mostly cleaning (not necessarily removing) _odd characters_ e.g. apostrophes, quotes, etc.
    - Sequences of characters: inline and End-Of-Line _ellipsis_, and other "non-sense".
    - Substitution on "|" that seem to be equivalent to end of sentences (i.e. a period).
    - Substitution of `<==/<--` and `==>/-->` with `;`.
    - Cleaning sequences of `!` and `?`.
- __Hashtags__: Recognized and replaced with a generic tag `HASHTAG` 
- __Acronyms__: limited to variations of `U.S.`, also replaced with a tag, `<USA>`.
- __Number-related__:
    + (likely) __dollar amounts__ by the presence of `$`: marked with `<MONEY>` tag.
    + __dates__ (_e.g. 12/34/5678_): marked with `<DATE>` tag.
    + __hours__ (_e.g. 1:30 p.m._): marked with `<HOUR>` tag.
    + _percentages_: marked with `<PERCENTAGE>` tag.
- __Repeated Consecutive Characters__: handled by type.  
    + `$` signs, assumed to stand for a money: replaced with tag `<MONEY>`.
    + `*`, within words usually are disguised profanities: replaced with `<PROFANITY>` tag.
    + `-`: context/surroundings dependent replacement with regular punctuation.
    + Some character sequences were entirely deleted: multiple `<`, `>`, `=`, `#`.
        

The dataset where cleaned with __perl scripts__, available on
[GitHub](https://github.com/pedrosan/DataScienceExamples/tree/master/myTextPredictr/MilestoneReport/scripts)
(look for `regularize_text-pass[1-5].pl` and `remove_tags_content.pl` scripts).   
A summary of what they do is listed in the [Appendix](#APPENDIX-regularize)


<a name="exclude_short_rows"></a>

### Excluding rows with less than 6 words

[Back to TOC](#CONTENT)

During my initial attempts it immediately emerged the problem of excessively short rows of text.
In particular, because I decided to perform tokenization on individual sentences, not directly on
individual rows, the tokenizer tripped and failed on empty "sentences" resulting from short 
rows.

I have then decided to set a cutoff to the minimum acceptable length of rows. 
After some empirical testing and row-length analysis with command line tools I have set a
threshold at $\ge6$ words.


<hr class="thin_separator">

<a name="LOADING"></a>

# MOVING TO R

<a name="LOADING_THE_DATA"></a>

## Loading the data

[Back to TOC](#CONTENT)

The rest of the analysis presented here is based on the _cleaned_ datasets resulting from
the processing described in the previous sections.

```{r load_REG_data, eval = FALSE, cache = TRUE}
# NOT EVALUATED because too computationally heavy (loading saved products)
in.blogs.REG   <- readByLine("./data/blogs_REG.txt.gz", check_nl = FALSE, skipNul = TRUE)
in.news.REG    <- readByLine("./data/news_REG.txt.gz", check_nl = FALSE, skipNul = TRUE)
in.twitter.REG <- readByLine("./data/twitter_REG.txt.gz", check_nl = FALSE, skipNul = TRUE)
```

Basic statistics of the three datasets in their original form:

<button class="toggle_code">show code</button>
```{r data_REG-basic_stats, eval = FALSE, cache = TRUE, echo = TRUE}
# NOT EVALUATED
stats.blogs   <- as.numeric(system("gzip -dc ./data/blogs_REG.txt.gz | wc | awk '{print $1; print $2; print $3}'", intern = TRUE))
stats.news    <- as.numeric(system("gzip -dc ./data/news_REG.txt.gz | wc | awk '{print $1; print $2; print $3}'", intern = TRUE))
stats.twitter <- as.numeric(system("gzip -dc ./data/twitter_REG.txt.gz | wc | awk '{print $1; print $2; print $3}'", intern = TRUE))

stats.REG.df <- data.frame( blogs = stats.blogs, news = stats.news, twitter = stats.twitter, 
                             row.names = c("lines", "words", "characters"), stringsAsFactors = FALSE)
saveRDS(stats.REG.df, "data/stats_REG.RDS")
```

```{r data_REG-basic_stats-read, eval = TRUE, echo = FALSE}
stats.REG.df <- readRDS("data/stats_REG.RDS")
```

After preprocessing we have the following stats:

```{r data_REG-basic_stats-kable, eval = TRUE, echo = FALSE}
kable(stats.REG.df)
```


<a name="FURTHER_CLEANING"></a>

## Further Data Cleaning in R

[Back to TOC](#CONTENT)

There are some common, customary, operations performed on a text dataset before proceeding to
analyze it.  

* __Make text lowercase__.
* __Strip extra white spaces__.
* __Remove numbers__.
* Remove punctuation.
* Remove _stopwords_.

Given that the goal is to __predict words in a typing context__ I think that removing
_stopwords_ does not make much sense.  
Working with a text without _stopwords_ may be useful if one wanted to use in the
prediction algorithm some information about words' association in sentences,
which may help improve meaningful discrimination between different _next word_
possibilities "proposed" by an algorithm based on _n-grams_.

Because of the context, I also do not think that removing punctuation would be wise, nor make
sense.  

```{r load_cleaned_data, eval = FALSE, cache = TRUE, echo = FALSE}
# LOADING SAVED PRODUCTS
# NOT ECHOED
in.blogs.REG <- readRDS("data/in.blogs.REG.RDS")
in.news.REG <- readRDS("data/in.news.REG.RDS")
in.twitter.REG <- readRDS("data/in.twitter.REG.RDS")
```

<a name="text_transformations"></a>

### Text transformations

[Back to TOC](#CONTENT)

The next step is applying the additional following three transformations:

* conversion to lower case.
* removal of numbers.
* removal of redundant white spaces.

Done as follows (with big obligatory acknowledgement and thank you to Hadley Wickham and Stefan
Bache for bringing us _the pipe_ `%>%`!).

```{r preproc-transform, eval = FALSE, cache = TRUE}
# NOT EVALUATED because too computationally heavy (loading saved products)
in.blogs.REG <- tolower(in.blogs.REG) %>% removeNumbers() %>% stripWhitespace()
in.news.REG <- tolower(in.news.REG) %>% removeNumbers() %>% stripWhitespace()
in.twitter.REG <- tolower(in.twitter.REG) %>% removeNumbers() %>% stripWhitespace()

# re-uppercases TAGS
in.blogs.REG <- gsub('<(emoticon|hashtag|dollaramount|period|hour|profanity|usa|percentage|date|money|ass|space|decade|ordinal|number|telephonenumber|timeinterval)>', 
                  '<\\U\\1>', in.blogs.REG, ignore.case = TRUE, perl = TRUE)
in.news.REG <- gsub('<(emoticon|hashtag|dollaramount|period|hour|profanity|usa|percentage|date|money|ass|space|decade|ordinal|number|telephonenumber|timeinterval)>', 
                  '<\\U\\1>', in.news.REG, ignore.case = TRUE, perl = TRUE)
in.twitter.REG <- gsub('<(emoticon|hashtag|dollaramount|period|hour|profanity|usa|percentage|date|money|ass|space|decade|ordinal|number|telephonenumber|timeinterval)>', 
                  '<\\U\\1>', in.twitter.REG, ignore.case = TRUE, perl = TRUE)
```


<hr class="thin_separator">

<a name="ANALYSIS"></a>

# ANALYSIS

<a name="ANALYSIS-STEP1"></a>

## Step 1 : Sentence Annotation (R)

[Back to TOC](#CONTENT)

As noted, after some tests, I settled on an approach whereby _n-grams tokenization_ is performed 
on separate individual sentences, instead of directly on individual rows as loaded from the dataset.

This is motivated by the fact that the _tokenizer_ I have adopted because I found its performance
to be more satisfactory, the `NGramTokenizer()` of the __RWeka__ package, does not seem to interrupt 
its construction of _n-grams_ at what are very likely sentence boundaries.

With _next word prediction_ in mind, it makes a lot of sense to restrict _n-grams_ to sequences
of words _within the boundaries of a sentence_.

Therefore, after cleaning, transforming and filtering the data, the first real operation 
I perform is the annotation of sentences, for which I have been using the __openNLP__ sentence
annotator `Maxent_Sent_Token_Annotator()`, with its default settings, and the function `annotate()` 
from the __NLP__ package.

```{r sentence_annotation-setup, eval = TRUE, cache = TRUE}
sent_token_annotator <- Maxent_Sent_Token_Annotator()
sent_token_annotator
```

I want the data in the form of a _vector with individual sentences_, and so I opted for `sapply()`
combined with a function wrapping the operations necessary to prepare a row of data for
annotation, the annotation itself and finally return a vector of sentences.

```{r show_find_sentences_function, eval = FALSE, echo = TRUE}
find_sentences <- function(x) {
    s <- paste(x, collapse = " ") %>% as.String()
    a <- NLP::annotate(s , sent_token_annotator) 
    as.vector(s[a])
}
```

To work around not fully performance issues, the tokenization was done on subsets (chunk)
of the data, comprising $100000$ lines.  Done this way was much faster than forcing the
tokenization over the 
The following code takes each dataset, and splits it into sentences chunk by chunk, writing out
the sentences for each chunk to separate files, which were then concatenated outside of R.

```{r sentence_annotation-run, eval = FALSE, cache = TRUE, echo = TRUE}
# NOT EVALUATED because too computationally heavy (loading saved products)
chunk_size <- 100000

for( what in c("blogs", "news", "twitter")) {
    
    data.what <- get(paste0("in.", what, ".REG"))
    len.what   <- length(data.what)
    cat(" - length ", len.what, "\n")
    
    n_chunks <- floor(len.what/chunk_size) + 1
    n1 <- ((1:n_chunks)-1)*chunk_size + 1
    n2 <- (1:n_chunks)*chunk_size
    n2[n_chunks] <- len.what
    Ns_by_chunk <- rep(0, n_chunks)
    print(n1)
    print(n2)
    
    names <- paste(what, "sentences", sprintf("%02d", (1:n_chunks)), sep = ".")
    fnames <- paste(names, ".gz", sep = "")
    print(names)
    print(fnames)
    
    # loop over chunks of size 'chunk_size'
    for(i in 1:n_chunks) {
        name1 <- names[i]
        fname1 <- fnames[i]
        idx <- n1[i]:n2[i]
        
        cat("   ", name1, length(idx), idx[1], idx[length(idx)], "\n")

        # find sentences
        assign( name1, sapply(data.what[idx], FUN = find_sentences, USE.NAMES = FALSE) %>% unlist )
        con <- gzfile(fname1, open = "w")

        # write sentences for this chunk to file
        writeLines(get(name1), con = con)
        close(con)
    }
    rm(i, n1, n2, n_chunks)
}
```

```{r load_sentences, eval = FALSE, cache = TRUE, echo = FALSE}
# NOT EVALUATED because too computationally heavy (loading saved products)
# NOT ECHOED
blogs.sentences   <- readByLine("./data/blogs.sentences.all.gz", check_nl = FALSE, skipNul = TRUE)
news.sentences    <- readByLine("./data/news.sentences.all.gz", check_nl = FALSE, skipNul = TRUE)
twitter.sentences <- readByLine("./data/twitter.sentences.all.gz", check_nl = FALSE, skipNul = TRUE)
```

```{r sentence_annotation-print_1, eval = TRUE, cache = TRUE, echo = FALSE}
# NOT ECHOED
N.sentences.blogs   <- as.numeric(system("gzip -dc ./data/blogs.sentences.ALL.gz | wc -l", intern = TRUE))
N.sentences.news    <- as.numeric(system("gzip -dc ./data/news.sentences.ALL.gz | wc -l", intern = TRUE))
N.sentences.twitter <- as.numeric(system("gzip -dc ./data/twitter.sentences.ALL.gz | wc -l", intern = TRUE))

N.sentences <- c(N.sentences.blogs, N.sentences.news, N.sentences.twitter)
stats.REG.df[4, ] <- N.sentences
stats.REG.df[5, ] <- round(stats.REG.df[4, ]/stats.REG.df[1, ], 3)
row.names(stats.REG.df)[4] <- "sentences"
row.names(stats.REG.df)[5] <- "sentences_per_line"
```

The stats table, with the added _number of sentences_  is now as follows:

```{r sentence_annotation-print_2, eval = TRUE, cache = TRUE, echo = FALSE}
for(i in 1:4){ stats.REG.df[i, ] <- sprintf("%s", stats.REG.df[i, ]) }
kable(stats.REG.df, align = "r", row.names = TRUE)
```

The list of sentences by dataset were then merged into a single master list.

### Some further cleaning of text (_perl_)

After the tokenization into sentences, some more fixes were applied with _perl scripts_ 
(sources posted on [GitHub](https://github.com/pedrosan/DataScienceExamples/tree/master/myTextPredictr/MilestoneReport/scripts)

```{r sentences_cleaning, engine = "bash", eval = FALSE}
gzip -dc all.sentences.ALL.gz | \
    ./sentences_cleaning-v1.pl | \
    awk '{if(NF > 1){print $0}}' | \
    ./sentences_cleaning-v2.pl | \
    ./sentences_cleaning-v2.pl | \
    ./sentences_cleaning-v2.pl | \
    ./sentences_cleaning-v3.pl | \
    ./sentences_cleaning-v2.pl
```

The repetition of script #2 turned out to be an easier approach instead of writing more
complex regular expressions.

The processed sentences were then filtered based on

* Number of words $\ge 3$ (`NF` in _awk_).
    * This criterion is simply due to the fact that we want sentences long enough to yield 3-grams.
* A _maximum_ value of a ratio variable defined as $\frac{(length - NF + 1)}{NF} \le 7.0$
    * This ratio cuts out particularly pathological sentences, including non-sense "words" (typically from twitter text).

```{r sentences_filtering_on_ratio, engine = "bash", eval = FALSE}
awk '{if(NF < 3){next}; ratio = (length - NF + 1)/NF; if(ratio > 7.0){next}; print $0}' 
```


### Removing _stop words_ (R)


```{r load_all_sentences, eval = FALSE, cache = TRUE, echo = FALSE}
# NOT ECHOED
all.sentences.ALL <- readByLine("./all.sentences.GO.gz", check_nl = FALSE, skipNul = TRUE)
```

After more thinking I decided to partially reconsider my decision against removal of _stop words_
and remove a controlled, selected list of them.

```{r remove_stopwords_from_sentences, eval = FALSE, cache = TRUE, echo = TRUE}
my_stop_words <- c("a", "an", "as", "at", "no", "of", "on", "or", 
                   "by", "so", "up", "or", "no", "in", "to", "rt")

# fixing extra spaces left by removing stop words
all.sentences <- removeWords(all.sentences.ALL, my_stop_words) %>% 
                        gsub(" +", " ", . , perl = TRUE) %>% 
                        gsub("^ +", "", . , perl = TRUE) %>% 
                        gsub(" +$", "", . , perl = TRUE)
```


<a name="ANALYSIS-STEP2"></a>

## Step 2 : _n-grams_ Tokenization

[Back to TOC](#CONTENT)

For the _n-grams_ tokenization I used the __RWeka Tokenizer__ `NGramTokenizer`, passing
to it a list of _token delimiters_.

I have not been able to run `NGramTokenizer()` on the full vector of sentences for each data set.
It fails on some variation of memory-allocation related error (that honestly does not make much
sense to me considering that I am running it on machines with 12GB of RAM).

So, I am processing data in chunks of 25,000 sentences, as _exemplified_ by this block of code
(the _n-grams_ data for the following section are loaded from saved previous analysis).

I extracted _n-grams_ for $n = 3, 4, 5$, with the code shown below:

<button class="toggle_code"><strong>show n-grams tokenization code</strong></button>
```{r tokenizing_ngrams, eval = FALSE, cache = TRUE, echo = TRUE}
token_delim <- " \\r\\n\\t.,;:\"()?!"
nl.chunk <- 25000

gc()
cat(" *** Tokenizing n-grams in WHOLE dataset [", my_date(), "]----------------------------------------\n")

len.all.sentences <- length(all.sentences)
cat(" *** Number of sentences in the WHOLE data set : ", len.all.sentences, "\n")

# define variable used to filter sentences long enough for n-grams of length N
subs <- strsplit(all.sentences, split = "[ ;,.\"\t\r\n()!?]+")
nstr.subs  <- sapply(subs, FUN = function(x) { length(unlist(x)) }, USE.NAMES = FALSE)
rm(subs)

for( ngram_size in 3:5 ) {
    cat(" *** Tokenizing : WHOLE : ", ngram_size, "-grams ----------------------------------------\n")
    
    good.sentences <- all.sentences[nstr.subs >= ngram_size]
    len.good <- length(good.sentences)
    cat("   Sentences with good length ( >=", ngram_size, ") : ", sprintf("%7d", len.good), "\n")
    cat("   Sentences with good length ( >=", ngram_size, ") : ", sprintf("%7d", len.good), 
           "(of ", sprintf("%7d", len.all.sentences), ")\n")

    n_chunks <- floor(len.good/nl.chunk) + 1
    n1 <- ((1:n_chunks)-1)*nl.chunk + 1
    n2 <- (1:n_chunks)*nl.chunk
    n2[n_chunks] <- len.good

    names <- paste("n", sprintf("%1d", ngram_size), "grams.blogs.", sprintf("%03d", (1:n_chunks)), sep = "")
    fnames <- paste("output/", names, ".gz", sep = "")
    
    for(i in 1:n_chunks) {
        name1 <- names[i]
        fname1 <- fnames[i]
        idx <- n1[i]:n2[i]
    
        cat("  [", sprintf("%3d", i), "/", sprintf("%3d", n_chunks), "]  ", 
               name1, length(idx), idx[1], idx[length(idx)], "\n")
    
        # tokenize to n-grams
        assign( name1, NGramTokenizer(good.sentences[idx], 
                Weka_control(min = ngram_size, max = ngram_size, delimiters = token_delim)) )
    
        # write to file n-grams from this chunk
        con <- gzfile(fname1, open = "w")
        writeLines(get(name1), con = con)
        close(con)

        gc()
    }

    # Combining chunks into one n-gram vector
    size.ngrams <- rep(0, n_chunks)
    total_length <- 0 
    for(i in 1:n_chunks) {
        name1 <- names[i]
        this_length <- length(get(name1))
        size.ngrams[i] <- this_length
        total_length <- total_length + this_length
        cat("  [", sprintf("%3d", i), "/", sprintf("%3d", n_chunks), 
               "]  length of ", name1, " = ", this_length, "\n")
    }
    cat("    Total Length = ", total_length, "\n")
    
    name_for_all_ngrams <- paste("n", sprintf("%1d", ngram_size), "grams.blogs.all", sep = "")
    temp_all_ngrams <- vector(mode = "character", length = total_length)
    ivec <- c(0, cumsum(size.ngrams))
    for(i in 1:n_chunks) {
        i1 <- ivec[i] + 1
        i2 <- ivec[i+1]
        name <- names[i]
        cat("   ", i, i1, i2, name, "\n")
        temp_all_ngrams[i1:i2] <- get(name)
    }

    assign( name_for_all_ngrams, temp_all_ngrams )

    # write to file all n-grams
    fname <- paste("output/", "n", sprintf("%1d", ngram_size), "grams.blogs.all.gz", sep = "")
    con <- gzfile(fname, open = "w")
    writeLines(temp_all_ngrams, con = con)
    close(con)

    # cleaning
    rm(good.sentences, len.good, temp_all_ngrams)
    
    rm(i, n1, n2, n_chunks)
    ls(pattern = "^n[1-6]grams.blogs.[0-9]")
    rm(list = ls(pattern = "^n[1-6]grams.blogs.[0-9]") )
    gc()
}
```

<a name="ngrams-cleaning_and_reclassification"></a>

### Cleaning and _re-classification_ of n-grams

A small fraction of the n-grams so produced contains no-ASCII characters, and it makes sense to
simply drop these n-grams.   
For instance with: 

```{r ngrams-remove_non_ascii, engine = "bash", eval = FALSE, echo = TRUE}
gzip -dc n5grams.all.gz | grep -P -v '([^\x00-\x7F]+)' 
```

Another _perl script_ (`ngrams-reprocess_clean_and_classify.pl`, find it on
[GitHub](https://github.com/pedrosan/DataScienceExamples/tree/master/myTextPredictr/MilestoneReport/scripts)) 
then

* Cleans up some problematic words or rejects some pathologically bad n-grams.
* Reclassifies n-grams based on their true adjusted number of words.

```{r ngrams-clean_and_reclassify, engine = "bash", eval = FALSE, echo = TRUE}
gzip -dc n5grams.filtered_for_nonASCII.gz | ./scripts/ngrams-reprocess_clean_and_classify.pl -go -print
```

This script produces a set of 7 files (`tmp_n*`) containing:
* good 1-grams
* good 2-grams
* good 3-grams
* good 4-grams
* good 5-grams
* n > 5 grams 
* Trashed n-grams (full of problem not worth dealing with)

Finally, we merge the n-grams of each order produced by the above script reprocessing all original n-grams.


<a name="ngrams_tables"></a>

### A look at the n-grams

[Back to TOC](#CONTENT)

__NOTE:__ the following results refer to the analysis of a subset of sentences (20%).

```{r load_ngrams, eval = FALSE, cache = TRUE, echo = FALSE}
# LOADING SAVED PRODUCTS
# NOT ECHOED
 load("data/n3grams.blogs.all.RData")
 load("data/n4grams.blogs.all.RData")
#
 load("data/n3grams.news.all.RData")
 load("data/n4grams.news.all.RData")
#
 load("data/n3grams.twitter.all.RData")
 load("data/n4grams.twitter.all.RData")

# load from scratch
# n3grams.blogs.all <- readByLine("data/n3grams.blogs.all.gz")
# n4grams.blogs.all <- readByLine("data/n4grams.blogs.all.gz")
# 
# n3grams.news.all <- readByLine("data/n3grams.news.all.gz")
# n4grams.news.all <- readByLine("data/n4grams.news.all.gz")
# 
# n3grams.twitter.all <- readByLine("data/n3grams.twitter.all.gz")
# n4grams.twitter.all <- readByLine("data/n4grams.twitter.all.gz")
```

```{r ngrams_stats, eval = FALSE, cache = TRUE, echo = FALSE}
n3grams.all <- c(n3grams.blogs.all, n3grams.news.all, n3grams.twitter.all)
n4grams.all <- c(n4grams.blogs.all, n4grams.news.all, n4grams.twitter.all)

N_n3g_blogs <- length(n3grams.blogs.all)
N_n3g_news <- length(n3grams.news.all)
N_n3g_twitter <- length(n3grams.twitter.all)
N_n3g_all <- length(n3grams.all)
N_n4g_blogs <- length(n4grams.blogs.all)
N_n4g_news <- length(n4grams.news.all)
N_n4g_twitter <- length(n4grams.twitter.all)
N_n4g_all <- length(n4grams.all)

N_unique_n3g_blogs <- length(unique(n3grams.blogs.all))
N_unique_n3g_news <- length(unique(n3grams.news.all))
N_unique_n3g_twitter <- length(unique(n3grams.twitter.all))
N_unique_n3g_all <- length(unique(n3grams.all))
N_unique_n4g_blogs <- length(unique(n4grams.blogs.all))
N_unique_n4g_news <- length(unique(n4grams.news.all))
N_unique_n4g_twitter <- length(unique(n4grams.twitter.all))
N_unique_n4g_all <- length(unique(n4grams.all))
```

```{r ngrams_stats-debug, eval = TRUE, cache = TRUE, echo = FALSE}
flag_debug <- FALSE
if( flag_debug == TRUE ) {
    N_n3g_blogs
    N_n3g_news
    N_n3g_twitter
    N_n3g_blogs + N_n3g_news + N_n3g_twitter
    N_unique_n3g_blogs
    N_unique_n3g_news
    N_unique_n3g_twitter
    N_unique_n3g_blogs + N_unique_n3g_news + N_unique_n3g_twitter
    N_unique_n3g_all
    N
    N_n4g_blogs
    N_n4g_news
    N_n4g_twitter
    N_n4g_blogs + N_n4g_news + N_n4g_twitter
    N_unique_n4g_blogs
    N_unique_n4g_news
    N_unique_n4g_twitter
    N_unique_n4g_blogs + N_unique_n4g_news + N_unique_n4g_twitter
    N_unique_n4g_all
}
```

From the _n-grams_ vectors we can compute frequencies, which will be an important basis for
the prediction algorithms.

For now we can take a peek at what are the most frequent _3-grams_ and _4-grams_ 
in the three datasets.

<button class="toggle_code">show code for blogs</button>
```{r ngrams_blogs_1, eval = FALSE, cache = TRUE, echo = TRUE}
n3g.blogs.freq <- as.data.frame(table(n3grams.blogs.all), stringsAsFactors = FALSE)
n3g.blogs.freq <- n3g.blogs.freq[order(n3g.blogs.freq$Freq, decreasing = TRUE), ]
row.names(n3g.blogs.freq) <- NULL

n4g.blogs.freq <- as.data.frame(table(n4grams.blogs.all), stringsAsFactors = FALSE)
n4g.blogs.freq <- n4g.blogs.freq[order(n4g.blogs.freq$Freq, decreasing = TRUE), ]
row.names(n4g.blogs.freq) <- NULL

colnames(n3g.blogs.freq) <- c("ngram", "count")
colnames(n4g.blogs.freq) <- c("ngram", "count")
```

<br />
<button class="toggle_code">show code for news</button>
```{r ngrams_news_1, eval = FALSE, cache = TRUE, echo = TRUE}
n3g.news.freq <- as.data.frame(table(n3grams.news.all), stringsAsFactors = FALSE)
n3g.news.freq <- n3g.news.freq[order(n3g.news.freq$Freq, decreasing = TRUE), ]
row.names(n3g.news.freq) <- NULL

n4g.news.freq <- as.data.frame(table(n4grams.news.all), stringsAsFactors = FALSE)
n4g.news.freq <- n4g.news.freq[order(n4g.news.freq$Freq, decreasing = TRUE), ]
row.names(n4g.news.freq) <- NULL

colnames(n3g.news.freq) <- c("ngram", "count")
colnames(n4g.news.freq) <- c("ngram", "count")
```

<br />
<button class="toggle_code">show code for twitter</button>
```{r ngrams_twitter_1, eval = FALSE, cache = TRUE, echo = TRUE}
n3g.twitter.freq <- as.data.frame(table(n3grams.twitter.all), stringsAsFactors = FALSE)
n3g.twitter.freq <- n3g.twitter.freq[order(n3g.twitter.freq$Freq, decreasing = TRUE), ]
row.names(n3g.twitter.freq) <- NULL

n4g.twitter.freq <- as.data.frame(table(n4grams.twitter.all), stringsAsFactors = FALSE)
n4g.twitter.freq <- n4g.twitter.freq[order(n4g.twitter.freq$Freq, decreasing = TRUE), ]
row.names(n4g.twitter.freq) <- NULL

colnames(n3g.twitter.freq) <- c("ngram", "count")
colnames(n4g.twitter.freq) <- c("ngram", "count")
```

```{r ngrams_top500, eval = FALSE, cache = TRUE, echo = FALSE}
n3g.blogs.top500 <- head(n3g.blogs.freq, 500)
n4g.blogs.top500 <- head(n4g.blogs.freq, 500)
n3g.news.top500 <- head(n3g.news.freq, 500)
n4g.news.top500 <- head(n4g.news.freq, 500)
n3g.twitter.top500 <- head(n3g.twitter.freq, 500)
n4g.twitter.top500 <- head(n4g.twitter.freq, 500)

nXg.all.top500 <- list(n3g.blogs.top500   = n3g.blogs.top500,
                       n4g.blogs.top500   = n4g.blogs.top500,
                       n3g.news.top500    = n3g.news.top500,
                       n4g.news.top500    = n4g.news.top500,
                       n3g.twitter.top500 = n3g.twitter.top500,
                       n4g.twitter.top500 = n4g.twitter.top500)

saveRDS(nXg.all.top500, file = "data/nXg_all_top500.RDS")
```

```{r ngrams_top500-read, eval = TRUE, cache = TRUE, echo = FALSE}
nXg.all.top500 <- readRDS("data/nXg_all_top500.RDS")

n3g.blogs.top500   <- nXg.all.top500$n3g.blogs.top500
n3g.news.top500    <- nXg.all.top500$n3g.news.top500
n3g.twitter.top500 <- nXg.all.top500$n3g.twitter.top500
n4g.blogs.top500   <- nXg.all.top500$n4g.blogs.top500
n4g.news.top500    <- nXg.all.top500$n4g.news.top500
n4g.twitter.top500 <- nXg.all.top500$n4g.twitter.top500
```

#### __3-grams__

<button class="toggle_code">show code</button>
```{r top20_n3grams-prepare, eval = TRUE, cache = TRUE, echo = TRUE}
tmp3.df <- cbind(head(n3g.blogs.top500, 20), 
                 head(n3g.news.top500, 20), 
                 head(n3g.twitter.top500, 20)) 
tmp4.df <- cbind(head(n4g.blogs.top500, 20), 
                 head(n4g.news.top500, 20), 
                 head(n4g.twitter.top500, 20)) 
colnames(tmp4.df) <- c("ngram_blogs", "count", "ngram_news", "count", "ngram_twitter", "count")
colnames(tmp4.df) <- c("ngram_blogs", "count", "ngram_news", "count", "ngram_twitter", "count")
# saveRDS(tmp3.df, file = "tmp_n3g_table.RDS")
```

* __blogs__

```{r top20_n3grams-print_blogs, eval = TRUE, echo = TRUE}
print(tmp3.df[, 1:2], print.gap = 3, right = FALSE)
```

* __news__

```{r top20_n3grams-print_news, eval = TRUE, echo = TRUE}
print(tmp3.df[, 3:4], print.gap = 3, right = FALSE)
```

* __twitter__

```{r top20_n3grams-print_twitter, eval = TRUE, echo = TRUE}
print(tmp3.df[, 5:6], print.gap = 3, right = FALSE)
```

```{r top20_n3grams-pander_2, eval = FALSE, cache = TRUE, echo = FALSE, results = 'asis'}
# NOT EVALUATED
# NOT ECHOED
panderOptions("table.split.table", Inf)
set.alignment(default = rep(c("left", "right"), 3))

pandoc.table(tmp3.df, style = "rmarkdown")
```

```{r top20_n3grams-kable_prepare_2, eval = FALSE, cache = TRUE, echo = FALSE}
# NOT EVALUATED
# NOT ECHOED
for( ic in c(1, 3, 5) ) {
    tmp3.df[, ic] <- gsub('<', '\&lt;', tmp3.df[, ic]) %>% gsub('>', '\&gt;', . )
}
```

```{r top20_n3grams-kable, eval = FALSE, cache = TRUE, echo = FALSE}
# NOT EVALUATED
# NOT ECHOED
kable(tmp3.df)
```

```{r top20_n3grams-pander_3, eval = FALSE, cache = TRUE, echo = FALSE, results = 'asis'}
# NOT EVALUATED
# NOT ECHOED
pandoc.table(tmp3.df, style = "rmarkdown")
```

#### __4-grams__

* __blogs__
```{r top20_n4grams-print_blogs, eval = TRUE, cache = TRUE}
print(tmp4.df[, 1:2], print.gap = 3, right = FALSE)
```

* __news__
```{r top20_n4grams-print_news, eval = TRUE, cache = TRUE}
print(tmp4.df[, 3:4], print.gap = 3, right = FALSE)
```

* __twitter__
```{r top20_n4grams-print_twitter, eval = TRUE, cache = TRUE}
print(tmp4.df[, 5:6], print.gap = 3, right = FALSE)
```

It is apparent that there some work will be necessary on the validation of the _n-grams_, 
or better still further text transformations, in particular of the _twitter_ data set that "suffers" 
from the tendency of using _shorthand slang_ (_e.g._ "rt" for "re-tweet") that adds a lot of "noise" 
to the data.

<a name="ngrams_plots"></a>

### Some Summary Plots for 4-grams

[Back to TOC](#CONTENT)

<a name="top30_mixed"></a>

#### Top-30 all mixed

```{r rank_mixed, eval = TRUE, cache = TRUE, echo = FALSE}
n4g.t <- subset(n4g.twitter.top500, count >= 50)
n4g.t$flag <- "twitter"
n4g.b <- subset(n4g.blogs.top500, count >= 50)
n4g.b$flag <- "blogs"
n4g.n <- subset(n4g.news.top500, count >= 50)
n4g.n$flag <- "news"

n4g.high <- rbind(n4g.b, n4g.n, n4g.t)
row.names(n4g.high) <- NULL
n4g.high.sorted <- n4g.high[order(n4g.high$count, decreasing = TRUE), ]
row.names(n4g.high.sorted) <- NULL
```

<button class="toggle_plot_code">show plot code</button>
```{r barplot_mix, eval = TRUE, cache = FALSE, echo = TRUE, fig.width = 10, fig.height = 7}
# ECHO FALSE
mycolors <- c("deepskyblue3", "firebrick2", "forestgreen")
data2pl <- n4g.high.sorted[1:30, ]
bp_mix <- ggplot(data2pl, aes(x = reorder(ngram, count), y = count)) + theme_bw() + coord_flip() + xlab("") + 
    theme(plot.title = element_text(face = "bold", size = 20)) + 
    theme(axis.text = element_text(size = 10)) + 
    scale_fill_manual(values = mycolors) + 
    ggtitle("Top 30 4-grams : all sources") + 
    geom_bar(stat = "identity", aes(fill = flag)) + 
    geom_text(aes(label = count, hjust = 1.1, size = 10), col = "white", position = position_stack())

bp_mix
```

<a name="top20_separate"></a>

#### Top-20 by data source

<button class="toggle_plot_code">show plot code</button>
```{r barplot_together, eval = TRUE, cache = FALSE, echo = TRUE, fig.width = 7, fig.height = 15}
# ECHO FALSE
data.blogs <- n4g.blogs.top500[1:20, ]
bp_blogs <- ggplot(data.blogs, aes(x = reorder(ngram, count), y = count)) + theme_bw() + coord_flip() + xlab("") + 
    theme(plot.title = element_text(face = "bold", size = 20)) + 
    theme(axis.text = element_text(size = 10)) + 
    theme(legend.position = "none") + 
    ggtitle("Top 20 4-grams : blogs") + 
    geom_bar(stat = "identity", fill = mycolors[1]) + 
    geom_text(aes(label = count, y = 100, size = 10), col = "white") 

data.news <- n4g.news.top500[1:20, ]
bp_news <- ggplot(data.news, aes(x = reorder(ngram, count), y = count)) + theme_bw() + coord_flip() + xlab("") + 
    theme(plot.title = element_text(face = "bold", size = 20)) + 
    theme(axis.text = element_text(size = 10)) + 
    theme(legend.position = "none") + 
    ggtitle("Top 20 4-grams : news") + 
    geom_bar(stat = "identity", fill = mycolors[2]) + 
    geom_text(aes(label = count, y = 100, size = 10), col = "white") 

data.twitter <- n4g.twitter.top500[1:20, ]
bp_twitter <- ggplot(data.twitter, aes(x = reorder(ngram, count), y = count)) + theme_bw() + coord_flip() + xlab("") + 
    theme(plot.title = element_text(face = "bold", size = 20)) + 
    theme(axis.text = element_text(size = 10)) + 
    theme(legend.position = "none") + 
    ggtitle("Top 20 4-grams : twitter") + 
    geom_bar(stat = "identity", fill = mycolors[3]) + 
    geom_text(aes(label = count, y = 100, size = 10), col = "white") 

grid.arrange(bp_blogs, bp_news, bp_twitter, nrow = 3)
```

<hr class="thin_separator">
<a name="APPENDIX"></a>

# APPENDIX 

[Back to the Top](#TOP)


<a name="APPENDIX-my_functions"></a>

## User Defined Functions

These are two handy functions used in the analysis.

* The first for reading the data.
* The second is passed to `sapply()` to annotate sentences, allowing to work by row instead of 
  converting the whole dataset into one document.

```{r read_my_functions, eval = TRUE, echo = FALSE, cache = FALSE}
read_chunk("./scripts/my_functions.R")
```

<button class="toggle_code">show code</button>
```{r print_chunk, eval = FALSE, echo = TRUE, cache = FALSE}
<<my_functions>>
```

More functions can be reviewed directly from the
[repository](https://github.com/pedrosan/DataScienceExamples/tree/master/myTextPredictr/MilestoneReport/scripts)


<a name="APPENDIX-regularize"></a>

## Summary of regularization done with _perl scripts_

Scripts source in [this GitHub folder](https://github.com/pedrosan/DataScienceExamples/tree/master/myTextPredictr/MilestoneReport/scripts)

#### _regularize_text-pass1.pl_

* CHARACTERS "HOMOGENEIZATION"
    * Normalize odd characters
    * Currency symbols
    * HTML tags
    * encoded apostrophe

* CHARACTER SEQUENCES
    * Cleaning of BEGIN / END of LINE
    * EOL ellipsis
    * EOL non-sense
    * word bracketed by *
    * substitution on "|" that seem to be equivalent to end of sentences (i.e. a period)
    * substitution of <==/<-- and ==>/--> with ";"
    * sequences of !, ? 

* HASHTAGS
    * recognized (most) hashtags


#### _regularize_text-pass2.pl_

* NUMBER RELATED
    * dates
    * hours am/pm
    * hours a.m./p.m.
    * dollar amounts
    * percentages

* ACRONYMS
    * acronyms: US

* EMOTICONS
    * regular 
    * (reverse) [not done]

* ELLIPSIS
    * INLINE ellipsis

* REPEATED CHARACTERS
    * DOLLAR
    * STAR
    * "+"
    * "-"
    * "a -- b" ==> replace with ","
    * "a--b"   ==> replace with ","
    * "! -- A" ==> REMOVE
    * "A--A[a ']" ==> replace with ";"
    * "a-- A" ==> replace with ";"
    * "a-- a" ==> replace with ","
    * LEAVE ONE (followed by space) : ,
    * LEAVE ONE : _  (the % is handled separately when dealing with percentages)
    * REMOVE ENTIRELY if 2+ : < > = #
   

#### _regularize_text-pass3.pl_

* CONTRACTIONS
    * 'll ==> _will / " will" ==> _will
    * n't ==> _not
    * 're ==> _are
    * 've ==> _have
    * some additional ad hoc (e.g. won't ==> will_not)
    * 's ==> _s
    * additional possibly useful/meaningful replacements (e.g. y'all)

* WHITE SPACES
    * squeezing extra white spaces
    * fixing some punctuation and white space

* PROFANITIES
    * catch the "7 ones" and replace with tag <PROFANITY>


#### _regularize_text-pass4.pl_

* ABBREVIATIONS: find and mark with tags standard/common abbreviations
    * month names
    * Mr, Mrs, Dr, ...

* MORE
    * Find and replace something like <NUMBER>-word
    * Remove genitives
    * Clean line endings preceded by spurious spaces
    * Replace: '' ==> "


#### _regularize_text-pass5.pl_

* WEIRD CHARACTERS
    * Replace weird characters with <WEIRDO> tag

* TAGS CLEANING
    * Clean consecutive <TAGS>
    * Remove <PERIOD>
    * Remove <SPACE>
    * Remove <WEIRDO>
    * Remove quotes from single quoted words

* MORE
    * Clear row BEGINNINGS with non-alpha characters

#### _regularize_text-pass6.pl_

* MORE TAG-RELATED REGULARIZATIONS
    * FIX missed HASHTAGS at line beginning
    * FIX additional number capture
    * Emptying tags that were defined to capture the original expression
        (e.g. <EMOJ_*_EMOJ> ==> <EMOTICON>


<a name="APPENDIX-post_sentence_cleaning"></a>

## Summary of post-sentence-tokenization cleaning (with _perl scripts_)

Scripts source in [this GitHub folder](https://github.com/pedrosan/DataScienceExamples/tree/master/myTextPredictr/MilestoneReport/scripts)

#### _sentences-cleaning_1.pl_

* CONTRACTIONS
    - 'll ==> _will / " will" ==> _will
    - n't ==> _not
    - 're ==> _are
    - 've ==> _have
    - some additional ad hoc
    - 's ==> _s
    - additional possibly useful/meaningful replacements (e.g. y'all)

#### _sentences-cleaning_2.pl_

* Remove spaces at the end of a sentence 
* More catching of profanities

* TAGS
    - Consecutive identical TAGS
    - Remove TAGS enclosed in parenthesis
    - Remove USELESS TAGS

* Remove excess space
    - Removed excess space at the beginning
    - Removed excess space at the end
    - Removed excess space in the middle

* Clean _non-alpha_ BEGINNING of sentences
    - Clean sentences "bracketed" by quotes or parenthesis (removing the "bracketing" character")
    - Remove beginning quotes not paired in the rest of the sentence.
    - Bracketed TAGS - just remove them
    - Non-alpha preceding a TAG (done separately to make life simpler)
    - Catching up with more fixes for beginning
    - Clean sentences "bracketed" by quotes or parenthesis (removing the "bracketing" character")

* Clean _non-alpha_ ENDING of sentences
    - Clean extra spaces before "good" sentence endings
    - Cleaning orphan " ' ) ] at the end match earlier in the sentence
    - Some dirty ENDS
    
#### _sentences-cleaning_3.pl_

* CLEAN (again) lines BEGINNING with TAGS and non-alpha



<a name="APPENDIX-tokenization_issue"></a>

### Mysterious issue with `NGramTokenizer` 

Because the `NGramTokenizer` would fail with a _java memory error_ if fed the full vector
of sentences, but run when fed chunks of 100,000 sentences, I thought that turning 
this into a basic loop handling the splitting in chunks, collecting the output and
finally return just one vector of _n-grams_ would work, be compact and smarter.

It turns out that it fails... and this puzzles me deeply.  
Is R somehow handling the "stuff" in the loop in the same way it would if I run
the tokenizer with the full vector?

Any clue?


```{r mystery_error, eval = FALSE}
# NOT EVALUATED
nl.chunk <- 100000
N <- ceiling(length(sel.blogs.sentences)/nl.chunk)
alt.n3grams.blogs <- vector("list", N)

system.time({
for( i in 1:N ) {
    i <- i+1
    n1 <- (i-1)*nl.chunk + 1
    n2 <- min(i*nl.chunk, end.blogs)
    cat(" ", i, n1, n2, "\n")
    alt.n3grams.blogs[[i]] <- NGramTokenizer(sel.blogs.sentences[n1:n2], 
                                             Weka_control(min = 3, max = 3, 
                                                          delimiters = token_delim)) 
}
})
```

---
